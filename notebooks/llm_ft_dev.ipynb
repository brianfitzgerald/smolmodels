{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from trl.trainer.dpo_trainer import DPOTrainer\n",
    "import gc\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "sys.path.append('..')\n",
    "from model.causal_lm import AutoLMFineTuner\n",
    "from model.utils import LMHyperParams, SmModel, ModelChoice\n",
    "from train import start_training\n",
    "from dataset.squad import UltraFeedbackDataModule\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = LMHyperParams(\n",
    "    base_model_checkpoint=\"Qwen/Qwen2.5-0.5B\",\n",
    "    warmup_steps_count=10,\n",
    "    tuning_type=\"dpo\",\n",
    "    train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optimizer=\"AdamW\",\n",
    ")\n",
    "\n",
    "# https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/dpo-align-llms-in-2024-with-trl.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoLMFineTuner(params)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "! echo $CUDA_VISIBLE_DEVICES\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-23 17:00:54.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mLoading dataset for stage fit\u001b[0m\n",
      "\u001b[32m2024-11-23 17:00:55.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m230\u001b[0m - \u001b[1mProcessing dataset for stage fit, workers: 12, cache dir dataset_caches/ultrafeedback\u001b[0m\n",
      "num_proc must be <= 9. Reducing num_proc to 9 for dataset of size 9.\n",
      "Parameter 'function'=<bound method UltraFeedbackDataModule.process_samples_batch of <dataset.squad.UltraFeedbackDataModule object at 0x1469a90766d0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map (num_proc=9): 100%|██████████| 9/9 [00:02<00:00,  3.56 examples/s]\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 61.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = UltraFeedbackDataModule(2, model.tokenizer, 1024, 10)\n",
    "if os.path.exists(data_module.cache_dir):\n",
    "    shutil.rmtree(data_module.cache_dir)\n",
    "data_module.cpu_count = 12\n",
    "data_module.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing reference logprobs:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing reference logprobs: 100%|██████████| 5/5 [00:21<00:00,  4.28s/it]\n",
      "\u001b[32m2024-11-23 17:16:15.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36mprecompute_reference_logprobs\u001b[0m:\u001b[36m303\u001b[0m - \u001b[1mConverting dataset\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_module.precompute_reference_logprobs(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_module.train_dataloader():\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to('cuda')\n",
    "    loss = model._step(batch)\n",
    "    print(loss.item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training(\n",
    "    \"16-mixed\", \"\", model, model.tokenizer, data_module, params, False, \"\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-brianf/smolmodels/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2024-11-22 23:04:20.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrain\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mLoading dependencies - Torch...\u001b[0m\n",
      "\u001b[32m2024-11-22 23:04:20.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrain\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mLoading dependencies - Lightning...\u001b[0m\n",
      "\u001b[32m2024-11-22 23:04:20.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrain\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mLoading dependencies - Project...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from trl.trainer.dpo_trainer import DPOTrainer\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "sys.path.append('..')\n",
    "from model.causal_lm import AutoLMFineTuner\n",
    "from model.utils import LMHyperParams, SmModel, ModelChoice\n",
    "from train import start_training\n",
    "from dataset.squad import UltraFeedbackDataModule\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = LMHyperParams(\n",
    "    base_model_checkpoint=\"Qwen/Qwen2.5-0.5B\",\n",
    "    warmup_steps_count=10,\n",
    "    tuning_type=\"dpo\",\n",
    "    train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optimizer=\"AdamW\",\n",
    ")\n",
    "\n",
    "# https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/dpo-align-llms-in-2024-with-trl.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-22 23:04:25.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.causal_lm\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mUsing DPO with LoraConfig: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=256, target_modules='all-linear', lora_alpha=128, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoLMFineTuner(\n",
       "  (model): PeftModelForCausalLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): Qwen2ForCausalLM(\n",
       "        (model): Qwen2Model(\n",
       "          (embed_tokens): Embedding(151936, 896)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x Qwen2DecoderLayer(\n",
       "              (self_attn): Qwen2SdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=896, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=256, out_features=896, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=896, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=256, out_features=128, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=896, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=256, out_features=128, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=896, out_features=896, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=896, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=256, out_features=896, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (rotary_emb): Qwen2RotaryEmbedding()\n",
       "              )\n",
       "              (mlp): Qwen2MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=896, out_features=4864, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=896, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=256, out_features=4864, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=896, out_features=4864, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=896, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=256, out_features=4864, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4864, out_features=896, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4864, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=256, out_features=896, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "              (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (reference_model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151936, 896)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "            (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "            (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "            (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoLMFineTuner(params)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Fri Nov 22 23:03:01 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:53:00.0 Off |                    0 |\n",
      "| N/A   25C    P0             110W / 700W |  48869MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  | 00000000:64:00.0 Off |                    0 |\n",
      "| N/A   23C    P0              70W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  | 00000000:75:00.0 Off |                    0 |\n",
      "| N/A   23C    P0              70W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              70W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  | 00000000:97:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             112W / 700W |  15474MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  | 00000000:A8:00.0 Off |                    0 |\n",
      "| N/A   24C    P0             108W / 700W |  39621MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  | 00000000:B9:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              67W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              69W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    701938      C   python3.9                                 48860MiB |\n",
      "|    4   N/A  N/A    120464      C   ...-brianf/smolmodels/.venv/bin/python     5110MiB |\n",
      "|    5   N/A  N/A   3176809      C   python3                                   39612MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "! echo $CUDA_VISIBLE_DEVICES\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-22 23:04:39.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mLoading dataset for stage fit\u001b[0m\n",
      "\u001b[32m2024-11-22 23:04:41.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m230\u001b[0m - \u001b[1mProcessing dataset for stage fit, workers: 12, cache dir dataset_caches/ultrafeedback\u001b[0m\n",
      "num_proc must be <= 9. Reducing num_proc to 9 for dataset of size 9.\n",
      "Parameter 'function'=<bound method UltraFeedbackDataModule.process_samples_batch of <dataset.squad.UltraFeedbackDataModule object at 0x155107dbfa50>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map (num_proc=9): 100%|██████████| 9/9 [00:01<00:00,  7.91 examples/s]\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 38.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = UltraFeedbackDataModule(2, model.tokenizer, 1024, 10)\n",
    "shutil.rmtree(data_module.cache_dir)\n",
    "data_module.cpu_count = 12\n",
    "data_module.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing reference logprobs: 100%|██████████| 5/5 [00:17<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'chosen_ref_log_probs': tensor([[[ 9.7230,  9.1477, 14.8778,  ..., -2.8170, -2.8184, -2.8170],\n",
      "         [10.6167,  9.2509, 16.3828,  ..., -2.6728, -2.6745, -2.6729],\n",
      "         [10.8379,  9.5980, 15.6573,  ..., -2.6257, -2.6273, -2.6258],\n",
      "         ...,\n",
      "         [ 5.8885,  0.5392,  4.4016,  ..., -4.1522, -4.1528, -4.1523],\n",
      "         [ 9.5476,  7.4200,  9.7727,  ..., -2.6856, -2.6859, -2.6855],\n",
      "         [-0.4611,  2.9680,  0.1975,  ...,  4.5102,  4.5104,  4.5102]],\n",
      "\n",
      "        [[13.1853, 12.3194, 16.9904,  ..., -2.6882, -2.6901, -2.6882],\n",
      "         [13.1143, 12.2432, 17.3645,  ..., -2.6545, -2.6562, -2.6546],\n",
      "         [12.1844, 12.3025, 16.6598,  ..., -2.6709, -2.6724, -2.6709],\n",
      "         ...,\n",
      "         [ 5.7268, -1.3190,  6.1402,  ..., -3.7019, -3.7019, -3.7020],\n",
      "         [ 8.8550,  6.0457, 10.3195,  ..., -2.2063, -2.2060, -2.2063],\n",
      "         [-3.4919, -4.1036, -1.2868,  ...,  6.0157,  6.0164,  6.0156]]]), 'rejected_ref_log_probs': tensor([[[ 9.7230,  9.1477, 14.8778,  ..., -2.8170, -2.8184, -2.8170],\n",
      "         [10.6167,  9.2509, 16.3828,  ..., -2.6728, -2.6745, -2.6729],\n",
      "         [10.8379,  9.5980, 15.6573,  ..., -2.6257, -2.6273, -2.6258],\n",
      "         ...,\n",
      "         [ 5.8885,  0.5392,  4.4016,  ..., -4.1522, -4.1528, -4.1523],\n",
      "         [ 9.5476,  7.4200,  9.7727,  ..., -2.6856, -2.6859, -2.6855],\n",
      "         [-0.4611,  2.9680,  0.1975,  ...,  4.5102,  4.5104,  4.5102]],\n",
      "\n",
      "        [[13.1853, 12.3194, 16.9904,  ..., -2.6882, -2.6901, -2.6882],\n",
      "         [13.1143, 12.2432, 17.3645,  ..., -2.6545, -2.6562, -2.6546],\n",
      "         [12.1844, 12.3025, 16.6598,  ..., -2.6709, -2.6724, -2.6709],\n",
      "         ...,\n",
      "         [ 5.7268, -1.3190,  6.1402,  ..., -3.7019, -3.7019, -3.7020],\n",
      "         [ 8.8550,  6.0457, 10.3195,  ..., -2.2063, -2.2060, -2.2063],\n",
      "         [-3.4919, -4.1036, -1.2868,  ...,  6.0157,  6.0164,  6.0156]]])}, {'chosen_ref_log_probs': tensor([[[12.3144, 10.6964, 16.2502,  ..., -3.0063, -3.0077, -3.0062],\n",
      "         [12.8137, 10.9899, 16.0587,  ..., -2.7857, -2.7873, -2.7856],\n",
      "         [12.5156, 10.9672, 15.1542,  ..., -2.7831, -2.7845, -2.7829],\n",
      "         ...,\n",
      "         [ 4.2433,  0.6466,  7.2199,  ..., -4.1137, -4.1141, -4.1141],\n",
      "         [ 9.5319,  7.9714, 11.3667,  ..., -2.0917, -2.0917, -2.0917],\n",
      "         [-3.0700, -1.5838, -0.9330,  ...,  5.5125,  5.5130,  5.5126]],\n",
      "\n",
      "        [[ 9.8045,  8.2456, 13.5470,  ..., -1.9674, -1.9680, -1.9672],\n",
      "         [ 9.9445,  8.1766, 13.3382,  ..., -2.0792, -2.0801, -2.0790],\n",
      "         [11.5273, 10.1849, 14.3350,  ..., -2.0356, -2.0366, -2.0354],\n",
      "         ...,\n",
      "         [ 6.5241,  1.0283,  6.0631,  ..., -5.4365, -5.4370, -5.4366],\n",
      "         [ 9.0122,  6.3527, 10.2237,  ..., -2.5657, -2.5659, -2.5656],\n",
      "         [ 1.3394, -0.1809,  1.2490,  ...,  4.0483,  4.0485,  4.0483]]]), 'rejected_ref_log_probs': tensor([[[12.3144, 10.6964, 16.2502,  ..., -3.0063, -3.0077, -3.0062],\n",
      "         [12.8137, 10.9899, 16.0587,  ..., -2.7857, -2.7873, -2.7856],\n",
      "         [12.5156, 10.9672, 15.1542,  ..., -2.7831, -2.7845, -2.7829],\n",
      "         ...,\n",
      "         [ 4.2433,  0.6466,  7.2199,  ..., -4.1137, -4.1141, -4.1141],\n",
      "         [ 9.5319,  7.9714, 11.3667,  ..., -2.0917, -2.0917, -2.0917],\n",
      "         [-3.0700, -1.5838, -0.9330,  ...,  5.5125,  5.5130,  5.5126]],\n",
      "\n",
      "        [[ 9.8045,  8.2456, 13.5470,  ..., -1.9674, -1.9680, -1.9672],\n",
      "         [ 9.9445,  8.1766, 13.3382,  ..., -2.0792, -2.0801, -2.0790],\n",
      "         [11.5273, 10.1849, 14.3350,  ..., -2.0356, -2.0366, -2.0354],\n",
      "         ...,\n",
      "         [ 6.5241,  1.0283,  6.0631,  ..., -5.4365, -5.4370, -5.4366],\n",
      "         [ 9.0122,  6.3527, 10.2237,  ..., -2.5657, -2.5659, -2.5656],\n",
      "         [ 1.3394, -0.1809,  1.2490,  ...,  4.0483,  4.0485,  4.0483]]])}, {'chosen_ref_log_probs': tensor([[[11.3986, 10.0781, 14.8239,  ..., -2.4324, -2.4339, -2.4324],\n",
      "         [11.3368, 10.1783, 14.7546,  ..., -2.5392, -2.5406, -2.5391],\n",
      "         [11.3185, 10.3228, 14.2440,  ..., -2.4286, -2.4298, -2.4285],\n",
      "         ...,\n",
      "         [ 5.3094, -1.7320,  5.8286,  ..., -4.3748, -4.3750, -4.3749],\n",
      "         [ 8.8490,  6.8287, 10.8707,  ..., -2.6904, -2.6902, -2.6904],\n",
      "         [ 0.8509,  1.6902,  1.5230,  ...,  3.7248,  3.7253,  3.7248]],\n",
      "\n",
      "        [[ 9.3486,  7.3796, 12.1949,  ..., -3.5068, -3.5079, -3.5063],\n",
      "         [10.0773,  8.6821, 14.4366,  ..., -3.4971, -3.4980, -3.4967],\n",
      "         [ 7.5432,  5.7774, 10.8838,  ..., -4.1234, -4.1244, -4.1230],\n",
      "         ...,\n",
      "         [ 5.3769,  1.6751,  7.7240,  ..., -4.8895, -4.8902, -4.8897],\n",
      "         [ 8.6557,  7.1440, 10.9374,  ..., -2.5735, -2.5737, -2.5735],\n",
      "         [-2.7074, -3.1814, -2.1912,  ...,  5.8053,  5.8061,  5.8052]]]), 'rejected_ref_log_probs': tensor([[[11.3986, 10.0781, 14.8239,  ..., -2.4324, -2.4339, -2.4324],\n",
      "         [11.3368, 10.1783, 14.7546,  ..., -2.5392, -2.5406, -2.5391],\n",
      "         [11.3185, 10.3228, 14.2440,  ..., -2.4286, -2.4298, -2.4285],\n",
      "         ...,\n",
      "         [ 5.3094, -1.7320,  5.8286,  ..., -4.3748, -4.3750, -4.3749],\n",
      "         [ 8.8490,  6.8287, 10.8707,  ..., -2.6904, -2.6902, -2.6904],\n",
      "         [ 0.8509,  1.6902,  1.5230,  ...,  3.7248,  3.7253,  3.7248]],\n",
      "\n",
      "        [[ 9.3486,  7.3796, 12.1949,  ..., -3.5068, -3.5079, -3.5063],\n",
      "         [10.0773,  8.6821, 14.4366,  ..., -3.4971, -3.4980, -3.4967],\n",
      "         [ 7.5432,  5.7774, 10.8838,  ..., -4.1234, -4.1244, -4.1230],\n",
      "         ...,\n",
      "         [ 5.3769,  1.6751,  7.7240,  ..., -4.8895, -4.8902, -4.8897],\n",
      "         [ 8.6557,  7.1440, 10.9374,  ..., -2.5735, -2.5737, -2.5735],\n",
      "         [-2.7074, -3.1814, -2.1912,  ...,  5.8053,  5.8061,  5.8052]]])}, {'chosen_ref_log_probs': tensor([[[ 8.8292,  7.7872, 11.9506,  ..., -4.0904, -4.0913, -4.0903],\n",
      "         [ 9.3057,  9.1188, 12.4437,  ..., -3.8953, -3.8961, -3.8952],\n",
      "         [ 9.7753, 10.0582, 13.3000,  ..., -3.5764, -3.5773, -3.5763],\n",
      "         ...,\n",
      "         [ 7.4507,  4.9918,  9.2956,  ..., -4.8023, -4.8022, -4.8025],\n",
      "         [ 9.4041,  7.3482, 10.8603,  ..., -1.5594, -1.5597, -1.5594],\n",
      "         [-2.0154, -2.5106, -1.5828,  ...,  6.1855,  6.1859,  6.1854]],\n",
      "\n",
      "        [[12.5716,  9.6828, 14.6335,  ..., -2.6087, -2.6100, -2.6084],\n",
      "         [12.2313,  9.5245, 13.9854,  ..., -2.8734, -2.8747, -2.8731],\n",
      "         [11.5378, 10.4990, 14.7928,  ..., -2.0766, -2.0778, -2.0763],\n",
      "         ...,\n",
      "         [ 6.7524,  1.2088,  8.2335,  ..., -4.6035, -4.6037, -4.6037],\n",
      "         [ 8.5601,  6.2423, 10.5215,  ..., -3.0308, -3.0307, -3.0307],\n",
      "         [ 2.5466,  2.0312,  3.4410,  ...,  1.8002,  1.8008,  1.8003]]]), 'rejected_ref_log_probs': tensor([[[ 8.8292,  7.7872, 11.9506,  ..., -4.0904, -4.0913, -4.0903],\n",
      "         [ 9.3057,  9.1188, 12.4437,  ..., -3.8953, -3.8961, -3.8952],\n",
      "         [ 9.7753, 10.0582, 13.3000,  ..., -3.5764, -3.5773, -3.5763],\n",
      "         ...,\n",
      "         [ 7.4507,  4.9918,  9.2956,  ..., -4.8023, -4.8022, -4.8025],\n",
      "         [ 9.4041,  7.3482, 10.8603,  ..., -1.5594, -1.5597, -1.5594],\n",
      "         [-2.0154, -2.5106, -1.5828,  ...,  6.1855,  6.1859,  6.1854]],\n",
      "\n",
      "        [[12.5716,  9.6828, 14.6335,  ..., -2.6087, -2.6100, -2.6084],\n",
      "         [12.2313,  9.5245, 13.9854,  ..., -2.8734, -2.8747, -2.8731],\n",
      "         [11.5378, 10.4990, 14.7928,  ..., -2.0766, -2.0778, -2.0763],\n",
      "         ...,\n",
      "         [ 6.7524,  1.2088,  8.2335,  ..., -4.6035, -4.6037, -4.6037],\n",
      "         [ 8.5601,  6.2423, 10.5215,  ..., -3.0308, -3.0307, -3.0307],\n",
      "         [ 2.5466,  2.0312,  3.4410,  ...,  1.8002,  1.8008,  1.8003]]])}, {'chosen_ref_log_probs': tensor([[[10.2349, 10.2003, 14.7750,  ..., -2.2756, -2.2772, -2.2756],\n",
      "         [11.1226,  9.9513, 15.6630,  ..., -2.6012, -2.6028, -2.6012],\n",
      "         [11.0048,  9.9557, 14.7554,  ..., -2.5990, -2.6004, -2.5989],\n",
      "         ...,\n",
      "         [ 6.5947,  3.7116,  9.0113,  ..., -4.9335, -4.9334, -4.9337],\n",
      "         [ 8.8578,  6.3700, 10.8453,  ..., -2.5641, -2.5637, -2.5641],\n",
      "         [-3.0823, -4.1757, -2.8648,  ...,  6.5257,  6.5260,  6.5256]]]), 'rejected_ref_log_probs': tensor([[[10.2349, 10.2003, 14.7750,  ..., -2.2756, -2.2772, -2.2756],\n",
      "         [11.1226,  9.9513, 15.6630,  ..., -2.6012, -2.6028, -2.6012],\n",
      "         [11.0048,  9.9557, 14.7554,  ..., -2.5990, -2.6004, -2.5989],\n",
      "         ...,\n",
      "         [ 6.5947,  3.7116,  9.0113,  ..., -4.9335, -4.9334, -4.9337],\n",
      "         [ 8.8578,  6.3700, 10.8453,  ..., -2.5641, -2.5637, -2.5641],\n",
      "         [-3.0823, -4.1757, -2.8648,  ...,  6.5257,  6.5260,  6.5256]]])}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (5/23 shards): 100%|██████████| 5/5 [00:09<00:00,  1.99s/ examples]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Index 5 out of range for dataset of size 5.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecompute_reference_logprobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/notebooks/../dataset/squad.py:305\u001b[0m, in \u001b[0;36mUltraFeedbackDataModule.precompute_reference_logprobs\u001b[0;34m(self, reference_model)\u001b[0m\n\u001b[1;32m    303\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_cached_logprobs \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(ldictl(samples)) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_cached_logprobs\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:1541\u001b[0m, in \u001b[0;36mDataset.save_to_disk\u001b[0;34m(self, dataset_path, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1541\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs_per_job\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_to_disk_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:1518\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1509\u001b[0m shards_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1510\u001b[0m pbar \u001b[38;5;241m=\u001b[39m hf_tqdm(\n\u001b[1;32m   1511\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1512\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   1513\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the dataset (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshards_done\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_shards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m shards)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1514\u001b[0m )\n\u001b[1;32m   1515\u001b[0m kwargs_per_job \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1516\u001b[0m     {\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: shard_idx,\n\u001b[0;32m-> 1518\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshard\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_shards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshard_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_idx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-of-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_shards\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.arrow\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1520\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: storage_options,\n\u001b[1;32m   1521\u001b[0m     }\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shard_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_shards)\n\u001b[1;32m   1523\u001b[0m )\n\u001b[1;32m   1524\u001b[0m shard_lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_shards\n\u001b[1;32m   1525\u001b[0m shard_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_shards\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:4698\u001b[0m, in \u001b[0;36mDataset.shard\u001b[0;34m(self, num_shards, index, contiguous, keep_in_memory, indices_cache_file_name, writer_batch_size)\u001b[0m\n\u001b[1;32m   4695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4696\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(index, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m), num_shards)\n\u001b[0;32m-> 4698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices_cache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices_cache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3848\u001b[0m, in \u001b[0;36mDataset.select\u001b[0;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[1;32m   3846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_range_contiguous(indices) \u001b[38;5;129;01mand\u001b[39;00m indices\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3847\u001b[0m         start, length \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m-\u001b[39m indices\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m-> 3848\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_contiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3850\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3908\u001b[0m, in \u001b[0;36mDataset._select_contiguous\u001b[0;34m(self, start, length, new_fingerprint)\u001b[0m\n\u001b[1;32m   3905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 3908\u001b[0m \u001b[43m_check_valid_indices_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3909\u001b[0m _check_valid_indices_value(start \u001b[38;5;241m+\u001b[39m length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m   3910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:622\u001b[0m, in \u001b[0;36m_check_valid_indices_value\u001b[0;34m(index, size)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_indices_value\u001b[39m(index, size):\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (index \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m size):\n\u001b[0;32m--> 622\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range for dataset of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Index 5 out of range for dataset of size 5."
     ]
    }
   ],
   "source": [
    "data_module.precompute_reference_logprobs(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_module.train_dataloader():\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to('cuda')\n",
    "    loss = model._step(batch)\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training(\n",
    "    \"16-mixed\", \"\", model, model.tokenizer, data_module, params, False, \"\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

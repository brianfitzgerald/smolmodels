{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from trl.trainer.dpo_trainer import DPOTrainer\n",
    "\n",
    "sys.path.append('..')\n",
    "from model.causal_lm import AutoLMFineTuner\n",
    "from model.utils import LMHyperParams, SmModel, ModelChoice\n",
    "from train import start_training\n",
    "from dataset.squad import UltraFeedbackDataModule\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = LMHyperParams(base_model_checkpoint=\"Qwen/Qwen2.5-0.5B\", warmup_steps_count=10, tuning_type=\"dpo\")\n",
    "\n",
    "# https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/dpo-align-llms-in-2024-with-trl.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoLMFineTuner(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-14 03:55:06.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m214\u001b[0m - \u001b[1mLoading dataset for stage fit\u001b[0m\n",
      "\u001b[32m2024-11-14 03:55:07.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mProcessing dataset for stage fit, workers: 12, cache dir dataset_caches/ultrafeedback\u001b[0m\n",
      "Map (num_proc=12): 100%|██████████| 60307/60307 [00:19<00:00, 3036.74 examples/s]\n",
      "Map (num_proc=12): 100%|██████████| 610/610 [00:03<00:00, 172.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = UltraFeedbackDataModule(16, model.tokenizer, 1024)\n",
    "data_module.cpu_count = 12\n",
    "data_module.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "\u001b[32m2024-11-14 03:55:45.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrain\u001b[0m:\u001b[36mstart_training\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1mEffective batch size: 8 (2x4)\u001b[0m\n",
      "/admin/home-brianf/smolmodels/.venv/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /admin/home-brianf/smolmodels/.venv/lib/python3.11/s ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m2024-11-14 03:55:45.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m214\u001b[0m - \u001b[1mLoading dataset for stage TrainerFn.FITTING\u001b[0m\n",
      "\u001b[32m2024-11-14 03:55:46.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mProcessing dataset for stage TrainerFn.FITTING, workers: 12, cache dir dataset_caches/ultrafeedback\u001b[0m\n",
      "Map (num_proc=12):  30%|██▉       | 18052/60307 [00:34<00:38, 1086.33 examples/s]"
     ]
    }
   ],
   "source": [
    "start_training(\n",
    "    \"16-mixed\", \"\", model, model.tokenizer, data_module, params, False, \"\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

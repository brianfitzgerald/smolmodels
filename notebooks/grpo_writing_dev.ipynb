{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from trl_wrapper.trainer_wrapper import TrainerWrapper, GRPO_WRITING_CONFIG, SMOL_LM_135M, TXT_BT_DPO_CONFIG\n",
    "\n",
    "cfg = TXT_BT_DPO_CONFIG\n",
    "cfg.train_batch_size = 2\n",
    "cfg.num_generations = 2\n",
    "cfg.model_id_or_path = SMOL_LM_135M\n",
    "cfg.notebook_mode = True\n",
    "cfg.max_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['scores', 'completion', 'instruction', 'model_id', 'prompt_id',\n",
      "       'instruction_id', 'score_total'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/6v9c2xns5lbc32tsmk83rh000000gn/T/ipykernel_12117/4268088263.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dataset_pd.groupby(\"instruction_id\").apply(_pick_completions).reset_index()\n"
     ]
    }
   ],
   "source": [
    "from dataset.writing import _sum_scores, _pick_completions\n",
    "import pandas as pd\n",
    "dataset_pd = pd.read_parquet(\"../dataset_files/backtranslate_best_of_n.parquet\")\n",
    "\n",
    "dataset_pd[\"instruction_id\"] = (\n",
    "    dataset_pd[\"instruction\"].astype(\"category\").cat.codes\n",
    ")\n",
    "\n",
    "dataset_pd[\"score_total\"] = dataset_pd[\"scores\"].apply(_sum_scores)\n",
    "\n",
    "dataset_pd = dataset_pd.groupby(\"instruction_id\").filter(\n",
    "    lambda group: group[\"score_total\"].max() != group[\"score_total\"].min()\n",
    ")\n",
    "print(dataset_pd.columns)\n",
    "dataset_pd = (\n",
    "    dataset_pd.groupby(\"instruction_id\").apply(_pick_completions).reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = TrainerWrapper(cfg)\n",
    "wrapper.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-01 22:28:52.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_data_module\u001b[0m:\u001b[36m351\u001b[0m - \u001b[1mUsing chat template override: smollmv2\u001b[0m\n",
      "\u001b[32m2025-04-01 22:28:52.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.wrapper_config\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mCache dir: ../dataset_caches/writing_d_p_o_data_module\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['scores', 'completion', 'instruction', 'model_id', 'prompt_id',\n",
      "       'instruction_id', 'score_total'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "wrapper.init_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.data_module.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-01 22:28:59.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m388\u001b[0m - \u001b[1mSaving output to: ./runs/04-01-22-28-940771-smollm2-135m-instruct--txt-bt-dpo\u001b[0m\n",
      "\u001b[32m2025-04-01 22:28:59.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m411\u001b[0m - \u001b[1mInitializing trainer, run_name: 04-01-22-28-940771-smollm2-135m-instruct--txt-bt-dpo, wandb project: gutenberg\u001b[0m\n",
      "\u001b[32m2025-04-01 22:28:59.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m418\u001b[0m - \u001b[1mlogprobs cache location: ../dataset_caches/writing_d_p_o_data_module/b68b303b/ref_logprobs_cache peft config: False\u001b[0m\n",
      "Extracting prompt in train dataset: 100%|██████████| 9299/9299 [00:01<00:00, 9274.34 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 9299/9299 [00:00<00:00, 12389.00 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 9299/9299 [00:18<00:00, 493.42 examples/s]\n",
      "Extracting prompt in eval dataset: 100%|██████████| 1034/1034 [00:00<00:00, 8973.96 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 1034/1034 [00:00<00:00, 11906.12 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1034/1034 [00:02<00:00, 513.38 examples/s]\n",
      "\u001b[32m2025-04-01 22:29:22.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1mPrecomputing reference logprobs, batch size: 16\u001b[0m\n",
      "\u001b[32m2025-04-01 22:29:22.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m644\u001b[0m - \u001b[1mPrecomputing train logprobs\u001b[0m\n",
      "Train dataset reference log probs:   0%|          | 0/582 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "wrapper.init_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.reasoning import GSM8K_SYSTEM_PROMPT, CONNECTIONS_PROMPT\n",
    "from model.utils import get_available_device\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": CONNECTIONS_PROMPT},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"train, panda, dove, series, wind, bear, orca, bass, string, skunk, speed, sand, zebra, tourist, desert, chain\",\n",
    "    },\n",
    "]\n",
    "tokenized_chat = wrapper.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "device = get_available_device()\n",
    "tokenized_chat = tokenized_chat.to(device)\n",
    "out = wrapper.model.generate(tokenized_chat, max_length=1024)\n",
    "print(wrapper.tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import display, Markdown\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for ReflectionCoder\n",
    "reflectioncoder_dataset = load_dataset(\"SenseLLM/ReflectionSeq-DS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_python_only(row):\n",
    "    for i, m in enumerate(row[\"messages\"]):\n",
    "        for j, part in enumerate(m[\"content\"]):\n",
    "            if \"python\" in part[\"content\"]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _explode_individual_messages(batch):\n",
    "    out = {\n",
    "        \"messages\": [],\n",
    "    }\n",
    "    for i, sample in enumerate(batch[\"messages\"]):\n",
    "        all_parts = []\n",
    "        for m in sample:\n",
    "            for j, part in enumerate(m[\"content\"]):\n",
    "                all_parts.append(\n",
    "                    {\n",
    "                        \"role\": m[\"role\"],\n",
    "                        \"content\": part[\"content\"],\n",
    "                        \"type\": part[\"type\"],\n",
    "                    }\n",
    "                )\n",
    "        out[\"messages\"].append(all_parts)\n",
    "    return out\n",
    "\n",
    "\n",
    "reflectioncoder_dataset = (\n",
    "    reflectioncoder_dataset[\"train\"]\n",
    "    .filter(_filter_python_only)\n",
    "    .map(_explode_individual_messages, batched=True)\n",
    "    .remove_columns([\"type\"])\n",
    ")\n",
    "ds_df = reflectioncoder_dataset.to_polars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _display_messages_markdown(df):\n",
    "    messages = df[\"messages\"]\n",
    "    for k, row in enumerate(messages):\n",
    "        display(Markdown(f\"# Row {k}\"))\n",
    "        for i, m in enumerate(row):\n",
    "            display(Markdown(f\"#### Message {i}: {m['role']}, {m['type']}\"))\n",
    "            content = m[\"content\"].replace(\"#\", \"\\#\").replace(\"\\n\", \"  \\n\")\n",
    "            display(Markdown(content))\n",
    "\n",
    "\n",
    "def _get_all_part_types(df):\n",
    "    type_cts = defaultdict(int)\n",
    "    for row in df[\"messages\"]:\n",
    "        for i, m in enumerate(row):\n",
    "            msg_type, msg_role = m[\"type\"], m[\"role\"]\n",
    "            type_cts[f\"{msg_role}_{msg_type}\"] += 1\n",
    "    return type_cts\n",
    "\n",
    "\n",
    "_display_messages_markdown(ds_df[0:2])\n",
    "# display(_get_all_part_types(ds_df[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "def process_reflection(sample: dict) -> Tuple[List[Dict[str, str]], Dict[str, str]]:\n",
    "    # these fields are misnamed in the dataset\n",
    "    for message in sample[\"messages\"]:\n",
    "        msg_role, msg_type = message[\"role\"], message[\"type\"]\n",
    "        role_out = \"assistant\"\n",
    "        print(message[\"role\"], message[\"type\"])\n",
    "\n",
    "    return (\n",
    "        conversation,\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": markdown_json(sample_data),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "display(process_reflection(reflectioncoder_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_dataset = load_dataset(\"glaiveai/reflection-v1\")\n",
    "reflection_dataset_pd = reflection_dataset[\"train\"].to_polars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "def _display_reflection_v1_markdown(df: pl.DataFrame):\n",
    "    for i, row in enumerate(df.to_dicts()):\n",
    "        display(Markdown(f\"# Row {i}\"))\n",
    "        display(Markdown(f\"#### system: {row['system']}\"))\n",
    "        display(Markdown(f\"#### Prompt: {row['prompt']}\"))\n",
    "        display(Markdown(f\"#### Response: {row['response']}\"))\n",
    "\n",
    "\n",
    "_display_reflection_v1_markdown(reflection_dataset_pd[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "humaneval_dataset = load_dataset(\"openai/openai_humaneval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from transformers.agents.python_interpreter import (\n",
    "    evaluate_python_code,\n",
    "    LIST_SAFE_MODULES,\n",
    ")\n",
    "import traceback\n",
    "\n",
    "ALLOWED_FNS = {\n",
    "    range,\n",
    "    print,\n",
    "    sum,\n",
    "    enumerate,\n",
    "    int,\n",
    "    str,\n",
    "    abs,\n",
    "    zip,\n",
    "    sorted,\n",
    "    list,\n",
    "    len,\n",
    "    bin,\n",
    "    isinstance,\n",
    "    set,\n",
    "    min,\n",
    "    max,\n",
    "    dict,\n",
    "    filter,\n",
    "    reversed,\n",
    "    chr,\n",
    "    ord,\n",
    "    tuple,\n",
    "    map,\n",
    "    round,\n",
    "}\n",
    "ALLOWED_FN_DICT = {fn.__name__: fn for fn in ALLOWED_FNS}\n",
    "\n",
    "# TODO execute tests\n",
    "failed = []\n",
    "\n",
    "SKIP_LIST = set(\n",
    "    [\n",
    "        \"HumanEval/32\",\n",
    "        \"HumanEval/38\",\n",
    "        \"HumanEval/50\",\n",
    "        \"HumanEval/75\",\n",
    "        \"HumanEval/99\",\n",
    "        \"HumanEval/104\",\n",
    "        \"HumanEval/105\",\n",
    "        \"HumanEval/126\",\n",
    "        \"HumanEval/137\",\n",
    "        \"HumanEval/145\",\n",
    "        \"HumanEval/150\",\n",
    "        \"HumanEval/160\",\n",
    "        \"HumanEval/162\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i, sample in enumerate(humaneval_dataset[\"test\"]):\n",
    "    prompt, solution, task_id = (\n",
    "        sample[\"prompt\"],\n",
    "        sample[\"canonical_solution\"],\n",
    "        sample[\"task_id\"],\n",
    "    )\n",
    "    tests, entrypoint = sample[\"test\"], sample[\"entry_point\"]\n",
    "    if task_id in SKIP_LIST:\n",
    "        continue\n",
    "    prompt = prompt.replace(\">>>\", \"\\n\")\n",
    "    tests = tests.replace(\"candidate(\", entrypoint + \"(\")\n",
    "    full_code = prompt + solution + tests + \"\\ncheck()\"\n",
    "    # display(Markdown(f\"```{full_code}```\"))\n",
    "    auth_imports = LIST_SAFE_MODULES + [\n",
    "        \"typing\",\n",
    "        \"copy\",\n",
    "        \"hashlib\",\n",
    "        \"string\",\n",
    "        \"collections\",\n",
    "    ]\n",
    "    # print(auth_imports)\n",
    "    try:\n",
    "        fn = evaluate_python_code(\n",
    "            full_code,\n",
    "            ALLOWED_FN_DICT,\n",
    "            authorized_imports=auth_imports,\n",
    "        )\n",
    "        print(f\"result for {i}: {fn}\")\n",
    "    except Exception as e:\n",
    "        # print(full_code)\n",
    "        # traceback.print_exc()\n",
    "        failed.append(sample[\"task_id\"])\n",
    "\n",
    "print(f\"{len(failed)} / {len(humaneval_dataset['test'])} failed\")\n",
    "print(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_dataset = load_dataset(\"deepmind/code_contests\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts = []\n",
    "for row in tqdm(cc_dataset):\n",
    "    n_public = len(row[\"public_tests\"][\"input\"])\n",
    "    n_private = len(row[\"private_tests\"][\"input\"])\n",
    "    test_counts.append((n_public, n_private))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts_np = np.array(test_counts)\n",
    "print(test_counts_np.mean(axis=0), test_counts_np.std(axis=0))\n",
    "\n",
    "plt.hist(test_counts_np[:, 0], bins=20)\n",
    "plt.show()\n",
    "plt.hist(test_counts_np[:, 1], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeforces_subset_pl = pl.read_parquet(\n",
    "    \"../dataset_samples/codeforces_problems_subset.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = codeforces_subset_pl[9:19][\"description\"].to_list()\n",
    "for i, desc in enumerate(descriptions):\n",
    "    display(Markdown(f\"## Description {i}\"))\n",
    "    display(Markdown(desc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

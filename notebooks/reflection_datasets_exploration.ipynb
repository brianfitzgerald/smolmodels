{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import polars as pl\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for ReflectionCoder\n",
    "reflectioncoder_dataset = load_dataset(\"SenseLLM/ReflectionSeq-DS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_python_only(row):\n",
    "    for i, m in enumerate(row['messages']):\n",
    "        for j, part in enumerate(m[\"content\"]):\n",
    "            if \"python\" in part[\"content\"]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def _explode_individual_messages(batch):\n",
    "    out = {\n",
    "        \"messages\": [],\n",
    "    }\n",
    "    for i, sample in enumerate(batch['messages']):\n",
    "        all_parts = []\n",
    "        for m in sample:\n",
    "            for j, part in enumerate(m[\"content\"]):\n",
    "                all_parts.append({\n",
    "                    \"role\": m[\"role\"],\n",
    "                    \"content\": part[\"content\"],\n",
    "                    'type': part['type']\n",
    "                })\n",
    "        out[\"messages\"].append(all_parts)\n",
    "    return out\n",
    "\n",
    "reflectioncoder_dataset = reflectioncoder_dataset['train'].filter(_filter_python_only).map(_explode_individual_messages, batched=True).remove_columns([\"type\"])\n",
    "ds_df = reflectioncoder_dataset.to_polars()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _display_messages_markdown(df):\n",
    "    messages = df[\"messages\"]\n",
    "    for k, row in enumerate(messages):\n",
    "        display(Markdown(f\"# Row {k}\"))\n",
    "        for i, m in enumerate(row):\n",
    "            display(Markdown(f\"#### Message {i}: {m['role']}, {m['type']}\"))\n",
    "            content = m[\"content\"].replace(\"#\", \"\\#\").replace(\"\\n\", \"  \\n\")\n",
    "            display(Markdown(content))\n",
    "\n",
    "\n",
    "def _get_all_part_types(df):\n",
    "    type_cts = defaultdict(int)\n",
    "    for row in df[\"messages\"]:\n",
    "        for i, m in enumerate(row):\n",
    "            msg_type, msg_role = m[\"type\"], m[\"role\"]\n",
    "            type_cts[f\"{msg_role}_{msg_type}\"] += 1\n",
    "    return type_cts\n",
    "\n",
    "\n",
    "# _display_messages_markdown(ds_df[0:1])\n",
    "display(_get_all_part_types(ds_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def process_reflection(sample: dict) -> Tuple[List[Dict[str, str]], Dict[str, str]]:\n",
    "\n",
    "    # these fields are misnamed in the dataset\n",
    "    for message in sample['messages']:\n",
    "        msg_role, msg_type = message['role'], message['type']\n",
    "        role_out = \"assistant\"\n",
    "        print(message['role'], message['type'])\n",
    "\n",
    "    return (\n",
    "        conversation,\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": markdown_json(sample_data),\n",
    "        },\n",
    "    )\n",
    "\n",
    "display(process_reflection(reflectioncoder_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_dataset = load_dataset(\"glaiveai/reflection-v1\")\n",
    "reflection_dataset_pd = reflection_dataset['train'].to_polars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def _display_reflection_v1_markdown(df: pl.DataFrame):\n",
    "    for i, row in enumerate(df.to_dicts()):\n",
    "        display(Markdown(f\"# Row {i}\"))\n",
    "        display(Markdown(f\"#### system: {row['system']}\"))\n",
    "        display(Markdown(f\"#### Prompt: {row['prompt']}\"))\n",
    "        display(Markdown(f\"#### Response: {row['response']}\"))\n",
    "_display_reflection_v1_markdown(reflection_dataset_pd[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "humaneval_dataset = load_dataset(\"openai/openai_humaneval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from transformers.agents.python_interpreter import (\n",
    "    evaluate_python_code,\n",
    "    LIST_SAFE_MODULES,\n",
    ")\n",
    "import traceback\n",
    "\n",
    "ALLOWED_FNS = {\n",
    "    range,\n",
    "    print,\n",
    "    sum,\n",
    "    enumerate,\n",
    "    int,\n",
    "    str,\n",
    "    abs,\n",
    "    zip,\n",
    "    sorted,\n",
    "    list,\n",
    "    len,\n",
    "    bin,\n",
    "    isinstance,\n",
    "    set,\n",
    "    min,\n",
    "    max,\n",
    "    dict,\n",
    "    filter,\n",
    "    reversed,\n",
    "    chr,\n",
    "    ord,\n",
    "    tuple,\n",
    "    map,\n",
    "    round\n",
    "}\n",
    "ALLOWED_FN_DICT = {fn.__name__: fn for fn in ALLOWED_FNS}\n",
    "\n",
    "# TODO execute tests\n",
    "failed = []\n",
    "\n",
    "SKIP_LIST = set(['HumanEval/32', 'HumanEval/38', 'HumanEval/50', 'HumanEval/75', 'HumanEval/99', 'HumanEval/104', 'HumanEval/105', 'HumanEval/126', 'HumanEval/137', 'HumanEval/145', 'HumanEval/150', 'HumanEval/160', 'HumanEval/162'])\n",
    "\n",
    "for i, sample in enumerate(humaneval_dataset[\"test\"]):\n",
    "    prompt, solution, task_id = sample[\"prompt\"], sample[\"canonical_solution\"], sample[\"task_id\"]\n",
    "    tests, entrypoint = sample[\"test\"], sample[\"entry_point\"]\n",
    "    if task_id in SKIP_LIST:\n",
    "        continue\n",
    "    prompt = prompt.replace(\">>>\", \"\\n\")\n",
    "    tests = tests.replace(\"candidate(\", entrypoint + \"(\")\n",
    "    full_code = prompt + solution + tests + \"\\ncheck()\"\n",
    "    # display(Markdown(f\"```{full_code}```\"))\n",
    "    auth_imports = LIST_SAFE_MODULES + [\"typing\", \"copy\", \"hashlib\", \"string\", \"collections\"]\n",
    "    # print(auth_imports)\n",
    "    try:\n",
    "        fn = evaluate_python_code(\n",
    "            full_code,\n",
    "            ALLOWED_FN_DICT,\n",
    "            authorized_imports=auth_imports,\n",
    "        )\n",
    "        print(f\"result for {i}: {fn}\")\n",
    "    except Exception as e:\n",
    "        # print(full_code)\n",
    "        # traceback.print_exc()\n",
    "        failed.append(sample['task_id'])\n",
    "\n",
    "print(f\"{len(failed)} / {len(humaneval_dataset['test'])} failed\")\n",
    "print(failed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "all_eval_csvs = []\n",
    "for root, dirs, files in os.walk(\"../eval_results\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            all_eval_csvs.append(os.path.join(root, file))\n",
    "\n",
    "all_eval_csvs = sorted(all_eval_csvs)\n",
    "\n",
    "df = pd.concat([pd.read_csv(f).assign(source=os.path.basename(os.path.dirname(f))) for f in all_eval_csvs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All criteria: {'Believable Character Actions', 'Emotionally Engaging', 'Coherent', 'Adherence to Instructions', 'Consistent Voice/Tone of Writing'}\n",
      "\n",
      "Average scores for eq_bench_writing_gpt-4.1-nano_04-21-20-39_3:\n",
      "Believable Character Actions: 15.89 ± 1.48\n",
      "Emotionally Engaging: 15.03 ± 1.74\n",
      "Coherent: 16.25 ± 1.26\n",
      "Adherence to Instructions: 17.68 ± 2.22\n",
      "Consistent Voice/Tone of Writing: 17.79 ± 0.74\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique criteria from the scores dictionaries\n",
    "import ast\n",
    "import statistics\n",
    "\n",
    "all_criteria = set()\n",
    "for scores in df['scores']:\n",
    "    scores = ast.literal_eval(scores)\n",
    "    all_criteria.update(scores.keys())\n",
    "\n",
    "print(\"All criteria:\", all_criteria)\n",
    "\n",
    "# Calculate average score for each criterion by source\n",
    "criteria_avgs_by_source = {}\n",
    "for source in df['source'].unique():\n",
    "    source_df = df[df['source'] == source]\n",
    "    criteria_avgs = {}\n",
    "    \n",
    "    for criterion in all_criteria:\n",
    "        scores = []\n",
    "        for score_dict in source_df['scores']:\n",
    "            score_dict = ast.literal_eval(score_dict)\n",
    "            if criterion in score_dict:\n",
    "                scores.append(score_dict[criterion])\n",
    "        if scores:\n",
    "            criteria_avgs[criterion] = sum(scores) / len(scores)\n",
    "    \n",
    "    criteria_avgs_by_source[source] = criteria_avgs\n",
    "# Print results by source\n",
    "for source, avgs in criteria_avgs_by_source.items():\n",
    "    print(f\"\\nAverage scores for {source}:\")\n",
    "    for criterion, avg in avgs.items():\n",
    "        # Get scores for this criterion and source\n",
    "        scores = []\n",
    "        for score_dict in df[df['source'] == source]['scores']:\n",
    "            score_dict = ast.literal_eval(score_dict)\n",
    "            if criterion in score_dict:\n",
    "                scores.append(score_dict[criterion])\n",
    "        stdev = statistics.stdev(scores) if len(scores) > 1 else 0\n",
    "        print(f\"{criterion}: {avg:.2f} ± {stdev:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

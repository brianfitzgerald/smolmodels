{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianfitzgerald/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-12 05:07:12,646\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from trl_wrapper.trainer_wrapper import TrainerWrapper, CONNECTIONS_CONFIG, SMOL_LM_135M\n",
    "\n",
    "cfg = CONNECTIONS_CONFIG\n",
    "cfg.train_batch_size = 2\n",
    "cfg.num_generations = 2\n",
    "cfg.model_id_or_path = SMOL_LM_135M\n",
    "cfg.notebook_mode = True\n",
    "cfg.max_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-12 05:07:16.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m285\u001b[0m - \u001b[1mSetting padding side to: right\u001b[0m\n",
      "\u001b[32m2025-04-12 05:07:16.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m296\u001b[0m - \u001b[1mUsing device: mps\u001b[0m\n",
      "\u001b[32m2025-04-12 05:07:16.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1mLoading model HuggingFaceTB/SmolLM2-135M-Instruct with attn_impl: sdpa\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wrapper = TrainerWrapper(cfg)\n",
    "wrapper.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.reasoning import ConnectionsDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-12 05:07:22.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.wrapper_config\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mCache dir: ../dataset_caches/connections_data_module\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conn = ConnectionsDataModule(wrapper.tokenizer, wrapper.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-12 05:07:39.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_data_module\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mUsing chat template override: smollmv2\u001b[0m\n",
      "\u001b[32m2025-04-12 05:07:39.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.wrapper_config\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mCache dir: ../dataset_caches/connections_data_module\u001b[0m\n",
      "Map: 100%|██████████| 100000/100000 [00:05<00:00, 18153.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "wrapper.init_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-12 05:12:01.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mSaving output to: ./runs/04-12-5-12-998514-smollm2-135m-instruct-\u001b[0m\n",
      "\u001b[32m2025-04-12 05:12:01.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m396\u001b[0m - \u001b[1mInitializing trainer, run_name: 04-12-5-12-998514-smollm2-135m-instruct-, wandb project: qwen-connections-grpo\u001b[0m\n",
      "\u001b[32m2025-04-12 05:12:01.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m479\u001b[0m - \u001b[1mUsing vllm: False\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wrapper.init_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.reasoning import GSM8K_SYSTEM_PROMPT, CONNECTIONS_PROMPT\n",
    "from model.utils import get_available_device\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": CONNECTIONS_PROMPT},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"train, panda, dove, series, wind, bear, orca, bass, string, skunk, speed, sand, zebra, tourist, desert, chain\",\n",
    "    },\n",
    "]\n",
    "tokenized_chat = wrapper.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "device = get_available_device()\n",
    "tokenized_chat = tokenized_chat.to(device)\n",
    "out = wrapper.model.generate(tokenized_chat, max_length=1024)\n",
    "print(wrapper.tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-12 05:20:31.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m644\u001b[0m - \u001b[1mStarting training.\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mXML count rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mStrict format rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36msoft_group_reward\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mSoft accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mGroup size rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mGenerations:\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mhair, makeup, props, wardrobe, mercury, sky, sparks, liberty, cent, chainz, pac, savage, digit, dog, mitt, piggy\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mhair, makeup, props, wardrobe, mercury, sky, sparks, liberty, cent, chainz, pac, savage, digit, dog, mitt, piggy\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>hair, makeup, props, wardrobe</group>\n",
      "<group>mercury, sky, sparks, liberty</group>\n",
      "<group>cent, chainz, pac, savage</group>\n",
      "<group>digit, dog, mitt, piggy</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mhair, makeup, props, wardrobe, mercury, sky, sparks, liberty, cent, chainz, pac, savage, digit, dog, mitt, piggy\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mI'd like to add a few more groups to the list.\n",
      "\n",
      "For \"hair,\" \"mask,\" \"manipulation,\" and \"hairpin\" are all related to hair, like \"hairpin,\" \"manipulation,\" \"hairpin,\" \"hairpin,\" \"hairpin,\" \"manipulation,\" \"hairpin,\" and \"hairpin.\"\n",
      "\n",
      "For \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bodice roulade,\" \"bod\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>hair, makeup, props, wardrobe</group>\n",
      "<group>mercury, sky, sparks, liberty</group>\n",
      "<group>cent, chainz, pac, savage</group>\n",
      "<group>digit, dog, mitt, piggy</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mHard accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:20:54.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mNumber of groups rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mXML count rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mStrict format rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36msoft_group_reward\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mSoft accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mGroup size rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mGenerations:\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mcorduroy, denim, linen, tweed, head, knees, shoulders, toes, ache, long, pine, thirst, cheese, color, hamster, prayer\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mYou've already identified the groups. Here's a breakdown of the connections:\n",
      "\n",
      "* \"courage\" and \"hope\" are related in the context of \"courage\" and \"hope\" in the context of \"courage and hope\" in the phrase \"courage and hope\".\n",
      "* \"courage\" and \"hope\" are related in the context of \"courage and hope\" in the phrase \"courage and hope\".\n",
      "* \"courage\" and \"hope\" are related in the context of \"courage and hope\" in the phrase \"courage and hope\".\n",
      "* \"courage\" and \"hope\" are related in the context of \"courage and hope\" in the phrase \"courage and hope\".\n",
      "\n",
      "So, the groups you've identified are:\n",
      "\n",
      "* \"courage\" and \"hope\" in the context of \"courage and hope\"\n",
      "* \"courage\" and \"hope\" in the context of \"courage and hope\"\n",
      "* \"courage\" and \"hope\" in the context of \"courage and hope\"\n",
      "* \"courage\" and \"hope\" in the context of \"courage and hope\"\n",
      "\n",
      "These connections help you identify the relationships between the words and make it easier to understand the context in which they're used.\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>corduroy, denim, linen, tweed</group>\n",
      "<group>head, knees, shoulders, toes</group>\n",
      "<group>ache, long, pine, thirst</group>\n",
      "<group>cheese, color, hamster, prayer</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mcorduroy, denim, linen, tweed, head, knees, shoulders, toes, ache, long, pine, thirst, cheese, color, hamster, prayer\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mYou've already identified the groups, but I can help with the next part:\n",
      "\n",
      "The words \"cordy,\" \"denim,\" \"linen,\" \"tweed,\" \"head,\" \"knees,\" \"toe,\" \"ache,\" \"long,\" \"pine,\" \"stupid,\" \"pray,\" and \"hope\" form a group.\n",
      "\n",
      "The words \"cordy,\" \"denim,\" \"linen,\" \"tweed,\" \"head,\" \"knees,\" \"toe,\" \"ache,\" \"long,\" \"pine,\" \"stupid,\" \"pray,\" and \"hope\" form another group.\n",
      "\n",
      "The words \"cordy,\" \"denim,\" \"linen,\" \"tweed,\" \"head,\" \"knees,\" \"toe,\" \"ache,\" \"long,\" \"pine,\" \"stupid,\" \"pray,\" and \"hope\" form a third group.\n",
      "\n",
      "The words \"cordy,\" \"denim,\" \"linen,\" \"tweed,\" \"head,\" \"knees,\" \"toe,\" \"ache,\" \"long,\" \"pine,\" \"stupid,\" \"pray,\" and \"hope\" form a fourth group.\n",
      "\n",
      "The words \"cordy,\" \"denim,\" \"linen,\" \"tweed,\" \"head,\" \"knees,\" \"toe,\" \"ache,\" \"long,\" \"pine,\" \"stupid,\" \"pray,\" and \"hope\" form a fifth group.\n",
      "\n",
      "The words \"cordy,\" \"denim,\" \"linen,\" \"tweed,\" \"head,\" \"knees,\" \"toe,\" \"ache,\" \"long,\" \"pine,\" \"stupid,\" \"pray,\" and \"hope\" form a sixth group.\n",
      "\n",
      "The words \"cordy,\" \"denim,\" \"linen,\" \"tweed,\" \"head,\" \"knees,\" \"toe,\" \"ache,\" \"long,\" \"pine,\" \"stupid,\" \"pray,\" and \"hope\" form a seventh group.\n",
      "\n",
      "The words \"cordy,\" \"denim,\" \"linen,\" \"tweed,\" \"head,\" \"knees,\" \"toe,\" \"ache,\" \"long,\" \"pine,\" \"stupid,\" \"pray,\" and \"hope\" form a eighth group.\n",
      "\n",
      "The words \"cordy,\" \"denim,\" \"linen,\" \"tweed,\" \"head,\" \"k\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>corduroy, denim, linen, tweed</group>\n",
      "<group>head, knees, shoulders, toes</group>\n",
      "<group>ache, long, pine, thirst</group>\n",
      "<group>cheese, color, hamster, prayer</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mHard accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:22.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mNumber of groups rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mXML count rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mStrict format rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36msoft_group_reward\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mSoft accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mGroup size rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mGenerations:\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mcopy, cut, paste, undo, char, eel, perch, shark, button, collar, cuff, pocket, date, dutch, jeopardy, space\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mcopy, cut, paste, undo, char, eel, perch, shark, button, collar, cuff, pocket, date, dutch, jeopardy, space\n",
      "\n",
      "I'm ready to help! What's your next question?\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>copy, cut, paste, undo</group>\n",
      "<group>char, eel, perch, shark</group>\n",
      "<group>button, collar, cuff, pocket</group>\n",
      "<group>date, dutch, jeopardy, space</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mcopy, cut, paste, undo, char, eel, perch, shark, button, collar, cuff, pocket, date, dutch, jeopardy, space\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mHere are the answers:\n",
      "\n",
      "# Example\n",
      "\n",
      "User: april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, april, a\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>copy, cut, paste, undo</group>\n",
      "<group>char, eel, perch, shark</group>\n",
      "<group>button, collar, cuff, pocket</group>\n",
      "<group>date, dutch, jeopardy, space</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mHard accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:21:47.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mNumber of groups rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mXML count rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mStrict format rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36msoft_group_reward\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mSoft accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mGroup size rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mGenerations:\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mdrop, splash, spot, sprinkle, felix, garfield, sylvester, tom, chic, hip, hot, in, clear, drain, empty, flush\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mThe words \"drop\", \"splash\", \"spot\", \"sniff\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"fleet\", \"\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>drop, splash, spot, sprinkle</group>\n",
      "<group>felix, garfield, sylvester, tom</group>\n",
      "<group>chic, hip, hot, in</group>\n",
      "<group>clear, drain, empty, flush</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mdrop, splash, spot, sprinkle, felix, garfield, sylvester, tom, chic, hip, hot, in, clear, drain, empty, flush\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mHere's a revised version of the sentence:\n",
      "\n",
      "\"Drop, splash, spot, sprinkle, felix, garfield, sylvester, tom, chic, hip, hot, in, clear, drain, empty, flush.\"\n",
      "\n",
      "I made a few changes to improve the clarity and flow of the sentence:\n",
      "\n",
      "- Changed \"drop, splash, spot, sprinkle, felix, garfield, sylvester, tom, chic, hip, hot, in\" to \"Drop, splash, spot, sprinkle, felix, garfield, sylvester, tom, chic, hip, hot, in\".\n",
      "- Added \"in\" before \"clear\" to make the sentence more idiomatic and clear.\n",
      "- Changed \"flush\" to \"empty\" to use a more common and common word in this context.\n",
      "\n",
      "The revised sentence is more concise and easier to read, making it more suitable for formal writing or professional communication.\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>drop, splash, spot, sprinkle</group>\n",
      "<group>felix, garfield, sylvester, tom</group>\n",
      "<group>chic, hip, hot, in</group>\n",
      "<group>clear, drain, empty, flush</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mHard accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:13.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mNumber of groups rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mXML count rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mStrict format rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36msoft_group_reward\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mSoft accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mGroup size rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mGenerations:\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1maction, camera, cut, lights, cardinal, lama, monk, pastor, belt, bracelet, tie, watch, channel, means, medium, vehicle\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mI'm glad you're enjoying the video. Let me rephrase the sentence for better clarity and flow:\n",
      "\n",
      "\"The action, camera, cuts, lights, cardinal, lama, monk, pastor, belt, bracelet, tie, watch, channel, means, medium, vehicle, is a combination of the actions of the action, camera, and the lighting of the action, which are all related to the action.\"\n",
      "\n",
      "I made a few changes to improve the sentence's clarity and flow:\n",
      "\n",
      "- Changed \"action\" to \"action\" to correct the possessive form.\n",
      "- Added \"combined\" to \"means\" to make the sentence more concise.\n",
      "- Changed \"lama\" to \"monk\" to correct the spelling error and add a bit more depth to the description of the monk.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Changed \"means\" to \"means\" to correct the spelling error.\n",
      "- Ch\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>action, camera, cut, lights</group>\n",
      "<group>cardinal, lama, monk, pastor</group>\n",
      "<group>belt, bracelet, tie, watch</group>\n",
      "<group>channel, means, medium, vehicle</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1maction, camera, cut, lights, cardinal, lama, monk, pastor, belt, bracelet, tie, watch, channel, means, medium, vehicle\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mYou've already identified the correct group: \"action\" and \"camera\" are related to \"action\", \"cut\", \"lights\", \"cardinal\", \"lama\", \"monk\", \"purchaser\", \"belt\", \"speaker\", \"means\", \"medium\", \"vehicle\", and \"channel\".\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>action, camera, cut, lights</group>\n",
      "<group>cardinal, lama, monk, pastor</group>\n",
      "<group>belt, bracelet, tie, watch</group>\n",
      "<group>channel, means, medium, vehicle</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mHard accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:38.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mNumber of groups rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mXML count rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mStrict format rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36msoft_group_reward\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mSoft accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mGroup size rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mGenerations:\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mlance, pin, skewer, spit, elegance, grace, style, taste, bull, crab, scales, twins, badger, bug, hound, nag\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mLance, pin, skewer, spit, elegance, grace, style, taste, bull, crab, scales, twins, badger, bug, hound, nag.\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>lance, pin, skewer, spit</group>\n",
      "<group>elegance, grace, style, taste</group>\n",
      "<group>bull, crab, scales, twins</group>\n",
      "<group>badger, bug, hound, nag</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mlance, pin, skewer, spit, elegance, grace, style, taste, bull, crab, scales, twins, badger, bug, hound, nag\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mLance, pin, skewer, spit, elegance, grace, style, taste, bull, crab, scales, twins, badger, bug, hound, nag.\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>lance, pin, skewer, spit</group>\n",
      "<group>elegance, grace, style, taste</group>\n",
      "<group>bull, crab, scales, twins</group>\n",
      "<group>badger, bug, hound, nag</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mHard accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:22:43.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mNumber of groups rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mXML count rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mStrict format rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36msoft_group_reward\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mSoft accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mGroup size rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mGenerations:\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mditto, likewise, same, second, gif, pdf, tiff, zip, apologies, oops, pardon, sorry, line, meter, rhyme, verse\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mSo, I have four groups of related words: \"candle\", \"crayon\", \"honeycomb\", and \"seal\". I'm thinking of \"teeth\", \"wax\", \"kettles\", and \"mittens\" as a possible groupings. Let's see if we can find a connection between these words: \"teeth\", \"wax\", \"kettles\", and \"mittens\" could form a group, like \"teeth, wax, kettles, and tusks\".\n",
      "\n",
      "I've identified a few clear groups: For \"teeth\", the group is \"crayon\", \"honeycomb\", and \"seal\". Then, for \"wax\", it pairs \"wax\", \"kettles\", and \"tusks\". For \"kettles\", \"mittens\", and \"raindrops\", it pairs \"mittens\", \"kettles\", and \"tusks\". For \"teeth\", \"wax\", \"kettles\", and \"mittens\", it pairs \"teeth\", \"wax\", \"kettles\", and \"tusks\".\n",
      "\n",
      "I've also identified a few groups: For \"teeth\", \"wax\", \"kettles\", and \"mittens\" could form a group, like \"teeth, wax, kettles, and tusks\". For \"teeth\", \"wax\", \"kettles\", and \"mittens\" could form a group, like \"teeth, wax, kettles, and tusks\".\n",
      "\n",
      "I've also identified a few groups: For \"teeth\", \"wax\", \"kettles\", and \"mittens\" could form a group, like \"teeth, wax, kettles, and tusks\". For \"teeth\", \"wax\", \"kettles\", and \"mittens\" could form a group, like \"teeth, wax, kettles, and tusks\".\n",
      "\n",
      "I've found four groups of related words: \"teeth\", \"wax\", \"kettles\", and \"mittens\". I'm thinking of \"teeth\", \"wax\", \"kettles\", and \"mittens\" as a possible groupings. Let's see if we can find a connection between these words: \"teeth\", \"wax\", \"kettles\", and \"mittens\" could form a group, like \"teeth, wax, kettles, and tusks\".\n",
      "\n",
      "I've identified a few clear\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>ditto, likewise, same, second</group>\n",
      "<group>gif, pdf, tiff, zip</group>\n",
      "<group>apologies, oops, pardon, sorry</group>\n",
      "<group>line, meter, rhyme, verse</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mditto, likewise, same, second, gif, pdf, tiff, zip, apologies, oops, pardon, sorry, line, meter, rhyme, verse\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mI see you've already identified the groups as \"canine\", \"fang\", \"molar\", \"tusk\", \"kettles\", \"mittens\", \"no\", \"no explanation\", \"no reason\", and \"no rhyme\". I see you've already identified the groups as \"canine\", \"fang\", \"molar\", \"tusk\", \"kettles\", \"mittens\", \"no\", \"no explanation\", \"no rhyme\", \"no reason\", and \"no rhyme\". I see you've already identified the groups as \"canine\", \"fang\", \"molar\", \"tusk\", \"kettles\", \"mittens\", \"no\", \"no explanation\", \"no rhyme\", \"no reason\", and \"no rhyme\". I see you've already identified the groups as \"canine\", \"fang\", \"molar\", \"tusk\", \"kettles\", \"mittens\", \"no\", \"no explanation\", \"no rhyme\", \"no reason\", and \"no rhyme\". I see you've already identified the groups as \"canine\", \"fang\", \"molar\", \"tusk\", \"kettles\", \"mittens\", \"no\", \"no explanation\", \"no rhyme\", \"no reason\", and \"no rhyme\". I see you've already identified the groups as \"canine\", \"fang\", \"molar\", \"tusk\", \"kettles\", \"mittens\", \"no\", \"no explanation\", \"no rhyme\", \"no reason\", and \"no rhyme\". I see you've already identified the groups as \"canine\", \"fang\", \"molar\", \"tusk\", \"kettles\", \"mittens\", \"no\", \"no explanation\", \"no rhyme\", \"no reason\", and \"no rhyme\". I see you've already identified the groups as \"canine\", \"fang\", \"molar\", \"tusk\", \"kettles\", \"mittens\", \"no\", \"no explanation\", \"no rhyme\", \"no reason\", and \"no rhyme\". I see you've already identified the groups as \"canine\", \"fang\", \"molar\", \"tusk\", \"kettles\", \"mittens\", \"no\", \"no explanation\", \"no rhyme\", \"no reason\", and \"no rhyme\". I see you've already identified the groups as \"canine\", \"fang\", \"molar\", \"tusk\", \"kettles\",\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>ditto, likewise, same, second</group>\n",
      "<group>gif, pdf, tiff, zip</group>\n",
      "<group>apologies, oops, pardon, sorry</group>\n",
      "<group>line, meter, rhyme, verse</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mHard accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:07.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mNumber of groups rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mXML count rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mStrict format rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36msoft_group_reward\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mSoft accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mGroup size rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mGenerations:\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mhat, shorts, sunglasses, tee, air, speak, state, voice, cover, jacket, page, spine, bed, chair, couch, table\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mhat, shorts, sunglasses, tee, air, speak, state, voice, cover, jacket, page, spine, bed, chair, couch, table\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>hat, shorts, sunglasses, tee</group>\n",
      "<group>air, speak, state, voice</group>\n",
      "<group>cover, jacket, page, spine</group>\n",
      "<group>bed, chair, couch, table</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mhat, shorts, sunglasses, tee, air, speak, state, voice, cover, jacket, page, spine, bed, chair, couch, table\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mHere is the list of words and their meanings:\n",
      "\n",
      "HAT, SHOES, SEAL, VEHICLE, COUPLET, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES, PACKAGE, CASE, BED, COURT, SHOES, EYE, SHOES, SHOES\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>hat, shorts, sunglasses, tee</group>\n",
      "<group>air, speak, state, voice</group>\n",
      "<group>cover, jacket, page, spine</group>\n",
      "<group>bed, chair, couch, table</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mHard accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:32.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mNumber of groups rewards: [0.0, 0.0]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='562500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     2/562500 : < :, Epoch 0.00/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-12 05:23:59.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mXML count rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mStrict format rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36msoft_group_reward\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mSoft accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mGroup size rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mGenerations:\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mfamily, flush, jelly, we, curses, darn, rats, shoot, atlas, hermes, nike, paris, cardamom, clove, coriander, cumin\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mFamily, flush, jelly, we, curses, darn, rats, shoot, atlas, hermes, nike, paris, cardamom, clove, coriander, cumin, darn, jellies, guns, arrows, arrows, guns, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows, arrows\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>family, flush, jelly, we</group>\n",
      "<group>curses, darn, rats, shoot</group>\n",
      "<group>atlas, hermes, nike, paris</group>\n",
      "<group>cardamom, clove, coriander, cumin</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mfamily, flush, jelly, we, curses, darn, rats, shoot, atlas, hermes, nike, paris, cardamom, clove, coriander, cumin\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mThe words are: \"family\", \"flush\", \"jelly\", \"we\", \"cursed\", \"rats\", \"shoot\", \"anatomy\", \"hermes\", \"nike\", \"paris\", \"cardamom\", \"clove\", \"crushes\", \"aspirin\", \"jelly\", \"socks\", \"jelly\", \"cookies\", \"socks\", \"jelly\", \"cinnamon\", \"cumin\", \"clove\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\", \"cumin\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>family, flush, jelly, we</group>\n",
      "<group>curses, darn, rats, shoot</group>\n",
      "<group>atlas, hermes, nike, paris</group>\n",
      "<group>cardamom, clove, coriander, cumin</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mHard accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:23:59.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mNumber of groups rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mXML count rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mStrict format rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36msoft_group_reward\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mSoft accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1mGroup size rewards: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mGenerations:\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1marm, branch, chapter, wing, shadow, tail, track, trail, cup, lid, stirrer, straw, face, mac, match, pocket\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mHere's a revised version of the sentence with some suggested edits:\n",
      "\n",
      "The branch of the tree was a long, thin branch that ran from the trunk to the ground. It was a branch that was long and thin, and it was also a branch that was long and thin.\n",
      "\n",
      "I made a few changes to improve clarity and flow. I changed \"arm\" to \"branch\" to use the correct term for a branch, and added \"and\" to connect the two clauses. I also changed \"branch\" to \"branch\" to use the correct plural form. Additionally, I changed \"wall\" to \"wall\" to use the correct plural form, and added \"and\" to connect the two clauses. Finally, I changed \"cup\" to \"cup\" to use the correct plural form, and added \"and\" to connect the two clauses.\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>arm, branch, chapter, wing</group>\n",
      "<group>shadow, tail, track, trail</group>\n",
      "<group>cup, lid, stirrer, straw</group>\n",
      "<group>face, mac, match, pocket</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mPrompt:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1marm, branch, chapter, wing, shadow, tail, track, trail, cup, lid, stirrer, straw, face, mac, match, pocket\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mGeneration:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mHere are the groups of words related to \"arm\", \"branch\", \"chapter\", \"wing\", \"shadow\", \"tail\", \"track\", \"cup\", \"lid\", \"stirrer\", \"pocket\", \"match\", \"cup\", \"lock\", \"cup\", \"cup\", \"lock\", \"cup\", \"lock\", \"cup\", \"lock\", \"cup\", \"lock\", \"cup\", \"lock\", \"cup\", \"lock\", \"cup\", \"lock\", \"cup\", \"lock\", \"cup\", \"lock\", \"cup\", \"lock\", \"cup\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\", \"lock\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mAnswer:--------------------\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m<answer><group>arm, branch, chapter, wing</group>\n",
      "<group>shadow, tail, track, trail</group>\n",
      "<group>cup, lid, stirrer, straw</group>\n",
      "<group>face, mac, match, pocket</group></answer>\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mhard_group_reward\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mHard accuracy scores: [0.0, 0.0]\u001b[0m\n",
      "\u001b[32m2025-04-12 05:24:26.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mNumber of groups rewards: [0.0, 0.0]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/notebooks/../trl_wrapper/trainer_wrapper.py:645\u001b[0m, in \u001b[0;36mTrainerWrapper.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    644\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2558\u001b[0m )\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/trainer.py:3730\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain):\n\u001b[1;32m   3728\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m-> 3730\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   3732\u001b[0m     loss_mb \u001b[38;5;241m=\u001b[39m smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/trl/extras/profiling.py:87\u001b[0m, in \u001b[0;36mprofiling_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:647\u001b[0m, in \u001b[0;36mGRPOTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 647\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffered_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps] \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:719\u001b[0m, in \u001b[0;36mGRPOTrainer._generate_and_score_completions\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;66;03m# Regular generation path\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, gather_deepspeed3_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mds3_gather_for_generation\n\u001b[1;32m    718\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[0;32m--> 719\u001b[0m         prompt_completion_ids \u001b[38;5;241m=\u001b[39m \u001b[43munwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# Compute prompt length and extract completion ids\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     prompt_length \u001b[38;5;241m=\u001b[39m prompt_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3434\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3434\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3438\u001b[0m     outputs,\n\u001b[1;32m   3439\u001b[0m     model_kwargs,\n\u001b[1;32m   3440\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3441\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    818\u001b[0m )\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:559\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[1;32m    573\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    581\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:489\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         )\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    492\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    493\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:264\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    261\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 264\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:334\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    333\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 334\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    337\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:172\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 172\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:432\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2380\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   2379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 2380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wrapper.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

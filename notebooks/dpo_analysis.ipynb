{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro, norm\n",
    "\n",
    "import os\n",
    "\n",
    "scores_df = pd.read_parquet(\n",
    "    os.path.expanduser(\"~/smolmodels/data/scored_sorted_logprobs.parquet\")\n",
    ")\n",
    "scores_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = scores_df[\n",
    "    [\n",
    "        \"chosen_logps\",\n",
    "        \"rejected_logps\",\n",
    "        \"mean_chosen_logits\",\n",
    "        \"mean_rejected_logits\",\n",
    "    ]\n",
    "]\n",
    "scores_df[\"logprobs_diff\"] = abs(\n",
    "    scores_df[\"chosen_logps\"] - scores_df[\"rejected_logps\"]\n",
    ")\n",
    "scores_df[\"logits_diff\"] = abs(\n",
    "    scores_df[\"mean_chosen_logits\"] - scores_df[\"mean_rejected_logits\"]\n",
    ")\n",
    "scores_df.hist(bins=100, figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate([\"logprobs_diff\", \"logits_diff\"]):\n",
    "    mean, std = scores_df[col].mean(), scores_df[col].std()\n",
    "    stat, p = shapiro(scores_df[col])\n",
    "    print(f\"Shapiro-Wilk score: {stat}, p-value: {p}\")\n",
    "    print(f\"{col} mean: {mean}, std: {std}\")\n",
    "    pdf_bins = [10, 50, 90, 100]\n",
    "    pdf_labels = [\"10-50\", \"50-90\", \"90-100\"]\n",
    "    percentile_bins = norm.ppf([x / 100 for x in pdf_bins], loc=mean, scale=std)\n",
    "    print(f\"{col} 10th, 50th, 90th percentiles: {percentile_bins}\")\n",
    "    scores_df[f\"{col}_percentile_idx\"] = pd.cut(\n",
    "        scores_df[col],\n",
    "        bins=percentile_bins,\n",
    "        labels=pdf_labels,\n",
    "        include_lowest=True,\n",
    "        right=True,\n",
    "    )\n",
    "    axs[i].hist(scores_df[col], bins=100)\n",
    "    axs[i].axvline(mean, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "    axs[i].axvline(mean + std, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "    axs[i].axvline(mean - std, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "    axs[i].set_title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.logprobs_diff.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "col = \"logprobs_diff\"\n",
    "mean, std = scores_df[col].mean(), scores_df[col].std()\n",
    "pdf_bins = [10, 50, 90, 100]\n",
    "pdf_labels = [\"10-50\", \"50-90\", \"90-100\"]\n",
    "percentile_bins = norm.ppf([x / 100 for x in pdf_bins], loc=mean, scale=std)\n",
    "print(f\"{col} 10th, 50th, 90th percentiles: {percentile_bins}\")\n",
    "scores_df[f\"{col}_percentile_idx\"] = pd.cut(\n",
    "    scores_df[col],\n",
    "    bins=percentile_bins,\n",
    "    labels=pdf_labels,\n",
    "    include_lowest=True,\n",
    "    right=True,\n",
    ")  # type: ignore\n",
    "ax.hist(scores_df[col], bins=100)\n",
    "ax.axvline(mean, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "ax.axvline(mean + std, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "ax.axvline(mean - std, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "ax.set_title(\"Histogram of per-sample logprob distances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MBPP = \"MBPP Pass@1\"\n",
    "HUMANEVAL = \"HumanEval Pass@1\"\n",
    "\n",
    "eval_results = [\n",
    "    {\"name\": \"Llama-3.1-8B-Instruct\", MBPP: 59.5, HUMANEVAL: 69.5},\n",
    "    {\"name\": \"DPO - Baseline\", MBPP: 63.1, HUMANEVAL: 72.4},\n",
    "    {\"name\": \"DPO - Curriculum Learning\", MBPP: 64.2, HUMANEVAL: 73.1},\n",
    "]\n",
    "\n",
    "eval_results_df = pd.DataFrame(eval_results)\n",
    "\n",
    "ax = eval_results_df.plot.bar(\n",
    "    x=\"name\", y=[MBPP, HUMANEVAL], figsize=(10, 5), title=\"Code Eval Results\"\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "ax.set_xlabel(\"\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_df_inverted = eval_results_df.copy()\n",
    "eval_results_df_inverted.set_index(\"name\", inplace=True)\n",
    "eval_results_df_inverted = eval_results_df_inverted.T\n",
    "ax = eval_results_df_inverted.plot(\n",
    "    kind=\"bar\", figsize=(10, 5), title=\"Code Eval Results\"\n",
    ")\n",
    "\n",
    "ax.set_ylim(50, 75)\n",
    "# Customize the plot\n",
    "plt.xticks(rotation=0)  # Keep group labels horizontal\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(title=\"Models\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.expanduser(\"~/blog/blog/2024-11-28-dpo-pruning/eval_results.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "sampled_codecontests_df = pd.read_parquet(\"../data/codecontests_dpo.parquet\")\n",
    "sampled_codecontests_df[\"logprobs_diff\"] = abs(\n",
    "    sampled_codecontests_df[\"chosen_logps\"] - sampled_codecontests_df[\"rejected_logps\"]\n",
    ")\n",
    "sampled_codecontests_df[\"logits_diff\"] = abs(\n",
    "    sampled_codecontests_df[\"mean_chosen_logits\"]\n",
    "    - sampled_codecontests_df[\"mean_rejected_logits\"]\n",
    ")\n",
    "sampled_codecontests_df.sort_values(\"logprobs_diff\", ascending=False, inplace=True)\n",
    "for i, sample in sampled_codecontests_df.tail(5).iterrows():\n",
    "    print(sample[\"logprobs_diff\"])\n",
    "    display(Markdown(f\"### Sample {i}\"))\n",
    "    display(Markdown(f\"```python\\n{sample['chosen']}\\n```\"))\n",
    "    display(Markdown(f\"```python\\n{sample['rejected']}\\n```\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

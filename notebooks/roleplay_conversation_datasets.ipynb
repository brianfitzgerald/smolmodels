{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import polars as pl\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "from typing import Sequence\n",
    "from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in [\n",
    "    # \"lara-martin/FIREBALL\",\n",
    "    # \"MinervaAI/Aesir-Preview\",\n",
    "    # \"hieunguyenminh/roleplay\",\n",
    "    # \"chargoddard/rpguild\",\n",
    "    # \"jondurbin/cinematika-v0.1\",\n",
    "    # \"codeparrot/apps\",\n",
    "    # \"glaiveai/glaive-code-assistant-v3\",\n",
    "    # \"jondurbin/py-dpo-v0.1\",\n",
    "    # \"lemonilia/roleplaying-forums-raw\",\n",
    "    # \"deepmind/code_contests\",\n",
    "    # \"SenseLLM/ReflectionSeq-DS\",\n",
    "    # \"openai/openai_humaneval\",\n",
    "    # \"argilla/distilabel-intel-orca-dpo-pairs\"\n",
    "    # not downloaded\n",
    "    # Squish42/bluemoon-fandom-1-1-rp-cleaned\n",
    "    \"SaylorTwift/Gutenberg\"\n",
    "]:\n",
    "    # run this fn to get the local directory of the dataset\n",
    "    snapshot_download(dataset_name, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = snapshot_download(\"lemonilia/roleplaying-forums-raw\", repo_type=\"dataset\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinematika_root_dir = snapshot_download(\n",
    "    \"jondurbin/cinematika-v0.1\", repo_type=\"dataset\"\n",
    ")\n",
    "os.listdir(cinematika_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(cinematika_root_dir):\n",
    "    file_path = os.path.join(cinematika_root_dir, filename)\n",
    "    _, extension = os.path.splitext(file_path)\n",
    "    print(filename)\n",
    "    if extension == \".parquet\":\n",
    "        dataset = Dataset.from_parquet(file_path)\n",
    "        print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_by_scene_dataset = Dataset.from_parquet(\n",
    "    os.path.join(cinematika_root_dir, \"scene_by_scene.parquet\")\n",
    ")\n",
    "actions_dataset = Dataset.from_parquet(\n",
    "    os.path.join(cinematika_root_dir, \"actions.parquet\")\n",
    ")\n",
    "plain_scenes_dataset = Dataset.from_parquet(\n",
    "    os.path.join(cinematika_root_dir, \"full_script.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_root_dir = snapshot_download(\"chargoddard/rpguild\", repo_type=\"dataset\")\n",
    "print(rp_root_dir)\n",
    "os.listdir(rp_root_dir)\n",
    "print(open(os.path.join(rp_root_dir, \"README.md\")).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_grammar_filtered_path = os.path.join(\n",
    "    rp_root_dir, \"grammar_filtered\", \"train-00000-of-00001.parquet\"\n",
    ")\n",
    "rp_grammar_filtered_dataset = pl.read_parquet(rp_grammar_filtered_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_grammar_filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def dictl(dict_of_lists: Dict[str, List]) -> Sequence[dict]:\n",
    "    \"\"\"\n",
    "    Dict of lists to list of dicts.\n",
    "    \"\"\"\n",
    "    return [dict(zip(dict_of_lists.keys(), t)) for t in zip(*dict_of_lists.values())]\n",
    "\n",
    "\n",
    "for i, sample in enumerate(rp_grammar_filtered_dataset.iter_rows(named=True)):\n",
    "    print(sample[\"username\"], sample[\"reply\"])\n",
    "    print(sample.keys())\n",
    "    display(Markdown(f\"### Sample {i}\"))\n",
    "    for msg in sample[\"context\"]:\n",
    "        display(Markdown(f\"**{msg['char_name']}**\\n\\n {msg['text']}\"))\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pippa_dataset = load_dataset(\n",
    "    \"PygmalionAI/PIPPA\", \"pippa_deduped\", trust_remote_code=True\n",
    ")\n",
    "pippa_dataset = pippa_dataset[\"train\"].to_polars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from typing import Literal, Optional, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DialogueLine:\n",
    "    character: str\n",
    "    content: str\n",
    "    is_human: bool\n",
    "\n",
    "\n",
    "def format_conversation_oai(sample: dict):\n",
    "    # TODO handle interpolation and character intros\n",
    "    conversation = sample[\"conversation\"]\n",
    "    description = sample[\"bot_description\"]\n",
    "    out_conv: Sequence[ChatCompletionMessageParam] = [\n",
    "        {\"role\": \"assistant\", \"content\": description}\n",
    "    ]\n",
    "    for msg, is_human in zip(conversation[\"message\"], conversation[\"is_human\"]):\n",
    "        out_conv.append({\"role\": \"user\" if is_human else \"assistant\", \"content\": msg})\n",
    "    print(out_conv)\n",
    "    return out_conv\n",
    "\n",
    "\n",
    "for sample in dataset_pl.iter_rows(named=True):\n",
    "    print(sample.keys())\n",
    "    format_conversation_oai(sample)\n",
    "    break\n",
    "\n",
    "all_convs_out = []\n",
    "for i, row in enumerate(tqdm(pippa_dataset.iter_rows(named=True))):\n",
    "    conv_in, bot_description, bot_name = (\n",
    "        row[\"conversation\"],\n",
    "        row[\"bot_description\"],\n",
    "        row[\"bot_name\"],\n",
    "    )\n",
    "    conv_out: list[ChatCompletionMessageParam] = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are talking to {bot_name}. {bot_description}\",\n",
    "        }\n",
    "    ]\n",
    "    for msg, is_human in zip(conv_in[\"message\"], conv_in[\"is_human\"]):\n",
    "        if is_human:\n",
    "            conv_out.append({\"role\": \"user\", \"content\": msg})\n",
    "        else:\n",
    "            conv_out.append({\"role\": \"assistant\", \"content\": f\"{bot_name}: {msg}\"})\n",
    "    # display(Markdown(f\"### Sample {i}\"))\n",
    "    # for msg in conv_out:\n",
    "    #     display(Markdown(f\"**{msg['role']}**\\n {msg['content']}\"))\n",
    "    all_convs_out.append(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame({\"conversation\": all_convs_out}).write_parquet(\"pippa_conversations.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from model.utils import LMHyperParams, SmModel, ModelChoice\n",
    "from synthetic_data.utils import dictl\n",
    "from dataset.squad import UltraFeedbackDataModule\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft.tuners.lora.config import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from typing import cast\n",
    "from peft.peft_model import PeftModel\n",
    "import gc\n",
    "from torch.amp.autocast_mode import autocast\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"qgallouedec/tiny-LlamaForCausalLM-3\"\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_id)  # type: ignore\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # to prevent errors with FA\n",
    "tokenizer.truncation_side = \"left\"  # to prevent cutting off last generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-24 17:06:11.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mLoading dataset for stage fit\u001b[0m\n",
      "\u001b[32m2024-11-24 17:06:12.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m224\u001b[0m - \u001b[1mLoaded dataset with 60917 samples\u001b[0m\n",
      "\u001b[32m2024-11-24 17:06:12.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mProcessing dataset for stage fit, workers: 1, cache dir dataset_caches/ultrafeedback\u001b[0m\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 5918.28 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 4362.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = UltraFeedbackDataModule(2, tokenizer, 1024, 1000, False)\n",
    "# debugger will fail without this\n",
    "data_module.num_workers = 1\n",
    "data_module.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'prompt', 'chosen', 'chosen-rating', 'chosen-model', 'rejected', 'rejected-rating', 'rejected-model'],\n",
       "    num_rows: 900\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-brianf/smolmodels/.venv/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Extracting prompt from train dataset: 100%|██████████| 900/900 [00:00<00:00, 2098.57 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 900/900 [00:00<00:00, 2347.77 examples/s]\n",
      "Extracting prompt from eval dataset: 100%|██████████| 100/100 [00:00<00:00, 1998.40 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 100/100 [00:00<00:00, 2005.45 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 900/900 [00:02<00:00, 435.83 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 100/100 [00:00<00:00, 414.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# max_prompt_length is the maximum length of the prompt and the max_length is the maximum length of the prompt + chosen or rejected response\n",
    "prompt_length = 1024\n",
    "max_seq_length = 1512\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    r=256,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "args = DPOConfig(\n",
    "    output_dir=\"../outputs\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=5e-5,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=700,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    # debugger will fail without this\n",
    "    dataloader_num_workers=0,\n",
    "    dataset_num_proc=1,\n",
    "    max_length=max_seq_length,\n",
    "    max_prompt_length=prompt_length,\n",
    "    precompute_ref_log_probs=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    beta=0.1,\n",
    "    loss_type=\"sigmoid\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,  # set to none since we use peft\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    train_dataset=data_module.train_dataset,\n",
    "    eval_dataset=data_module.val_dataset,\n",
    "    tokenizer=tokenizer,  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer._peft_has_been_casted_to_bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing logprobs:   0%|          | 0/900 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing logprobs:   2%|▏         | 16/900 [00:15<17:27,  1.19s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from trl.trainer.dpo_trainer import PreferenceCollator\n",
    "\n",
    "\n",
    "def get_sample_wise_metrics(model, batch):\n",
    "    metrics = {}\n",
    "\n",
    "    model_output = trainer.concatenated_forward(model, batch)\n",
    "\n",
    "    # if ref_chosen_logps and ref_rejected_logps in batch use them, otherwise use the reference model\n",
    "    if \"ref_chosen_logps\" in batch and \"ref_rejected_logps\" in batch:\n",
    "        ref_chosen_logps = batch[\"ref_chosen_logps\"]\n",
    "        ref_rejected_logps = batch[\"ref_rejected_logps\"]\n",
    "    else:\n",
    "        ref_chosen_logps, ref_rejected_logps = trainer.compute_ref_log_probs(batch)\n",
    "\n",
    "    losses, chosen_rewards, rejected_rewards = trainer.dpo_loss(\n",
    "        model_output[\"chosen_logps\"],\n",
    "        model_output[\"rejected_logps\"],\n",
    "        ref_chosen_logps,\n",
    "        ref_rejected_logps,\n",
    "    )\n",
    "    reward_accuracies = (chosen_rewards > rejected_rewards).float()\n",
    "    reward_margins = chosen_rewards - rejected_rewards\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": losses.tolist(),\n",
    "        \"reward_accuracy\": reward_accuracies.tolist(),\n",
    "        \"reward_margin\": reward_margins.tolist(),\n",
    "    }\n",
    "\n",
    "    for k in [\n",
    "        \"chosen_logps\",\n",
    "        \"rejected_logps\",\n",
    "    ]:\n",
    "        metrics[k] = model_output[k].tolist()\n",
    "    return metrics\n",
    "\n",
    "\n",
    "outputs = []\n",
    "\n",
    "# call this to precompute logprobs\n",
    "trainer.get_train_dataloader()\n",
    "collator = PreferenceCollator(tokenizer.pad_token_id, \"pt\")  # type: ignore\n",
    "batch_size = 1\n",
    "with torch.no_grad(), autocast(\"cuda\"):\n",
    "    for batch in tqdm(\n",
    "        trainer.train_dataset.iter(batch_size=batch_size),\n",
    "        desc=\"Precomputing logprobs\",\n",
    "        total=len(trainer.train_dataset) // batch_size,\n",
    "    ):\n",
    "        sample_collated = collator(dictl(batch))\n",
    "        metrics = get_sample_wise_metrics(model, sample_collated)\n",
    "        for i in range(batch_size):\n",
    "            out_sample = {\n",
    "                \"prompt\": batch[\"prompt\"][i],\n",
    "                \"chosen\": batch[\"chosen\"][i],\n",
    "                \"rejected\": batch[\"rejected\"][i],\n",
    "            }\n",
    "            for k, v in metrics.items():\n",
    "                if isinstance(v, list):\n",
    "                    out_sample[k] = v[i]\n",
    "                else:\n",
    "                    out_sample[k] = v\n",
    "            outputs.append(out_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.693359375,\n",
       "  0.693359375,\n",
       "  0.693359375,\n",
       "  0.693359375,\n",
       "  0.693359375,\n",
       "  0.693359375,\n",
       "  0.693359375,\n",
       "  0.693359375],\n",
       " 'reward_accuracy': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'reward_margin': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'chosen_logps': [-303.25,\n",
       "  -659.0,\n",
       "  -253.125,\n",
       "  -448.0,\n",
       "  -324.0,\n",
       "  -278.25,\n",
       "  -1229.0,\n",
       "  -738.5],\n",
       " 'rejected_logps': [-215.75,\n",
       "  -521.0,\n",
       "  -373.75,\n",
       "  -1092.0,\n",
       "  -176.5,\n",
       "  -614.5,\n",
       "  -357.75,\n",
       "  -911.0],\n",
       " 'mean_chosen_logits': 1.333984375,\n",
       " 'mean_rejected_logits': 1.4111328125}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributiuon of losses\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for metric in outputs[0]['metrics']:\n",
    "    values = [x['metrics'][metric] for x in outputs]\n",
    "    plt.hist(values, bins=50)\n",
    "    plt.title(f'{metric} distribution')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

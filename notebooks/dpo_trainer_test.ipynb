{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-brianf/smolmodels/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import gc\n",
    "sys.path.append('..')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from trl_wrapper.trainer_wrapper import TrainerWrapper, LLAMA_CONFIG, SMOL_LM_135M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-18 16:16:27.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mLoading model HuggingFaceTB/SmolLM2-135M-Instruct\u001b[0m\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import replace\n",
    "cfg = replace(LLAMA_CONFIG, model_id_or_path=SMOL_LM_135M, max_samples=100, single_process_mode=True)\n",
    "wrapper = TrainerWrapper(cfg)\n",
    "wrapper.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-18 16:18:18.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m101\u001b[0m - \u001b[1mLoading dataset for stage fit\u001b[0m\n",
      "\u001b[32m2024-12-18 16:18:18.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mProcessing dataset for stage fit, workers: 1, cache dir dataset_caches/ultrafeedback\u001b[0m\n",
      "\u001b[32m2024-12-18 16:18:20.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.squad\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m248\u001b[0m - \u001b[1mLoaded dataset with 60917 samples\u001b[0m\n",
      "\u001b[32m2024-12-18 16:18:20.349\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m144\u001b[0m - \u001b[33m\u001b[1mComputed columns not found in dataset: ['source', 'prompt', 'chosen', 'chosen-rating', 'chosen-model', 'rejected', 'rejected-rating', 'rejected-model'] assuming using \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wrapper.init_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "! echo $CUDA_VISIBLE_DEVICES\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-18 16:29:41.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoRA already loaded, ignoring\u001b[0m\n",
      "\u001b[32m2024-12-18 16:29:41.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mInitializing DPOTrainer, with project name: codecontests-llama-3b, run_name: run-390448, logprobs cache: dataset_caches/ultrafeedback/b68b303b/ref_logprobs_cache peft config: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=512, target_modules='dummy-target-modules', exclude_modules=None, lora_alpha=256, lora_dropout=0.2, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\u001b[0m\n",
      "\u001b[32m2024-12-18 16:29:41.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m212\u001b[0m - \u001b[1mLoading cached logprobs...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wrapper.init_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-18 16:33:08.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m318\u001b[0m - \u001b[1mStarting training.\u001b[0m\n",
      "\u001b[32m2024-12-18 16:33:09.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mevaluation_loop\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mGenerating samples...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:33:09.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mgenerate_from_model_and_ref\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mGenerating policy samples, max length: 512...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:33:14.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mgenerate_from_model_and_ref\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating reference samples, max length: 512...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:33:24.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mevaluation_loop\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mSaved eval samples.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='339' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 339/2250 03:12 < 18:10, 1.75 it/s, Epoch 1.50/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653750</td>\n",
       "      <td>-4.064411</td>\n",
       "      <td>-5.750903</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>1.686492</td>\n",
       "      <td>-638.796814</td>\n",
       "      <td>-480.255951</td>\n",
       "      <td>1.565012</td>\n",
       "      <td>1.365926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.597481</td>\n",
       "      <td>-3.729437</td>\n",
       "      <td>-5.371728</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.642292</td>\n",
       "      <td>-635.447021</td>\n",
       "      <td>-476.464172</td>\n",
       "      <td>1.367066</td>\n",
       "      <td>1.144184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.655248</td>\n",
       "      <td>-6.093868</td>\n",
       "      <td>-8.334324</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>2.240455</td>\n",
       "      <td>-659.091431</td>\n",
       "      <td>-506.090149</td>\n",
       "      <td>0.940810</td>\n",
       "      <td>0.586920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.845491</td>\n",
       "      <td>-8.670111</td>\n",
       "      <td>-11.608720</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>2.938608</td>\n",
       "      <td>-684.853821</td>\n",
       "      <td>-538.834045</td>\n",
       "      <td>0.430990</td>\n",
       "      <td>0.096374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-18 16:34:08.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mevaluation_loop\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mGenerating samples...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:34:08.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mgenerate_from_model_and_ref\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mGenerating policy samples, max length: 512...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:34:13.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mgenerate_from_model_and_ref\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating reference samples, max length: 512...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:34:23.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mevaluation_loop\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mSaved eval samples.\u001b[0m\n",
      "\u001b[32m2024-12-18 16:35:07.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mevaluation_loop\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mGenerating samples...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:35:07.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mgenerate_from_model_and_ref\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mGenerating policy samples, max length: 512...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:35:12.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mgenerate_from_model_and_ref\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating reference samples, max length: 512...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:35:22.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mevaluation_loop\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mSaved eval samples.\u001b[0m\n",
      "/admin/home-brianf/smolmodels/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-12-18 16:36:08.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mevaluation_loop\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mGenerating samples...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:36:08.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mgenerate_from_model_and_ref\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mGenerating policy samples, max length: 512...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:36:13.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mgenerate_from_model_and_ref\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating reference samples, max length: 512...\u001b[0m\n",
      "\u001b[32m2024-12-18 16:36:23.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.dpo_trainer\u001b[0m:\u001b[36mevaluation_loop\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mSaved eval samples.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/notebooks/../trl_wrapper/trainer_wrapper.py:319\u001b[0m, in \u001b[0;36mTrainerWrapper.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    318\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/transformers/trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2248\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wrapper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "for prompt, chosen, rejected, logprob_diff in []:\n",
    "    display(Markdown(f\"\\n### Prompt: {prompt}\\n\\n### Chosen:\\n {chosen}\\n\\n### Rejected:\\n {rejected}\\n\\nLogprob diff: {logprob_diff}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

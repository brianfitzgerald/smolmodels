{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import gc\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from train_trl import TrainerWrapper, WrapperConfig, LLAMA_3_2_1B, SMOL_LM_135M\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "project_dir = os.path.abspath(os.path.join(\".\", os.pardir))\n",
    "print(project_dir)\n",
    "cfg = WrapperConfig(\n",
    "    single_process_mode=True,\n",
    "    model_id=LLAMA_3_2_1B,\n",
    "    using_filtered_logprobs=False,\n",
    "    root_dir=project_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = TrainerWrapper(cfg)\n",
    "wrapper.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.init_data_module(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.tokenizer.eos_token, wrapper.tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.init_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "! echo $CUDA_VISIBLE_DEVICES\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO why does bs>1 still sum? is from concat_fwd?\n",
    "outputs = wrapper.compute_loss_metrics(1)\n",
    "# need to use no_grad or get OOM\n",
    "# first_batch = next(iter(wrapper.trainer.get_train_dataloader()))\n",
    "# print(wrapper.tokenizer.decode(first_batch['prompt_input_ids'][0]))\n",
    "# with torch.no_grad():\n",
    "#     for batch in wrapper.trainer.get_train_dataloader():\n",
    "#         loss, out = wrapper.trainer.compute_loss(wrapper.model, batch, True)\n",
    "#         print(loss)\n",
    "#         display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.DataFrame(outputs)\n",
    "outputs.to_parquet(\"codecontests_dpo.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.read_parquet('dpo_scores.parquet')\n",
    "# out_df = pd.DataFrame(outputs)\n",
    "# out_df.to_parquet('out_df.parquet')\n",
    "out_df.head()\n",
    "\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of losses\n",
    "\n",
    "STR_COLS = ['prompt', 'chosen', 'rejected']\n",
    "ZERO_COLS = ['reward_accuracy', 'reward_margin', 'chosen_rewards', 'rejected_rewards', \"loss\"]\n",
    "\n",
    "plot_cols = [col for col in out_df.columns if col not in STR_COLS + ZERO_COLS]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(plot_cols), figsize=(25, 5))\n",
    "\n",
    "for i, col in enumerate(plot_cols):\n",
    "    if col in STR_COLS or col in ZERO_COLS:\n",
    "        continue\n",
    "    axs[i].hist(out_df[col], bins=50)\n",
    "    axs[i].set_title(col)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_cols)\n",
    "logprob_differences = out_df['chosen_logps'] - out_df['rejected_logps']\n",
    "out_df['logprob_differences'] = logprob_differences\n",
    "plt.hist(logprob_differences, bins=50)\n",
    "plt.title('logprob_differences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "samples_sorted_logprob_diff = out_df.sort_values(\n",
    "    \"logprob_differences\", ascending=False\n",
    ")\n",
    "samples_sorted_highest_diff = samples_sorted_logprob_diff[[\"prompt\", \"chosen\", \"rejected\", \"logprob_differences\"]].head(10)\n",
    "for prompt, chosen, rejected, logprob_diff in samples_sorted_highest_diff.values:\n",
    "    display(Markdown(f\"\\n### Prompt: {prompt}\\n\\n### Chosen:\\n {chosen}\\n\\n### Rejected:\\n {rejected}\\n\\nLogprob diff: {logprob_diff}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "Dataset.from_pandas(out_df).to_parquet('dpo_scores_sorted.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from model.utils import LMHyperParams, SmModel, ModelChoice\n",
    "from dataset.squad import UltraFeedbackDataModule\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft.tuners.lora.config import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from typing import cast\n",
    "from peft.peft_model import PeftModel\n",
    "import gc\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\"  # replace with your model id\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_id)  # type: ignore\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # to prevent errors with FA\n",
    "tokenizer.truncation_side = \"left\"  # to prevent cutting off last generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = UltraFeedbackDataModule(2, tokenizer, 1024, 100, False)\n",
    "# debugger will fail without this\n",
    "data_module.num_workers = 1\n",
    "data_module.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_prompt_length is the maximum length of the prompt and the max_length is the maximum length of the prompt + chosen or rejected response\n",
    "prompt_length = 1024\n",
    "max_seq_length = 1512\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    r=256,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "args = DPOConfig(\n",
    "    output_dir=\"doplhin-dpo\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=5e-5,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=700,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    # debugger will fail without this\n",
    "    dataloader_num_workers=0,\n",
    "    dataset_num_proc=1,\n",
    "    max_length=max_seq_length,\n",
    "    max_prompt_length=prompt_length,\n",
    "    precompute_ref_log_probs=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    beta=0.1,\n",
    "    loss_type=\"sigmoid\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,  # set to none since we use peft\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    train_dataset=data_module.train_dataset,\n",
    "    eval_dataset=data_module.val_dataset,\n",
    "    tokenizer=tokenizer,  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure not to create a computation graph, or the model will OOM\n",
    "with torch.no_grad():\n",
    "    for batch in trainer.get_train_dataloader():\n",
    "        loss, metrics = trainer.compute_loss(model, batch, True)\n",
    "        print(loss)\n",
    "        display(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

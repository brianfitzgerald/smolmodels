{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-brianf/smolmodels/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import gc\n",
    "sys.path.append('..')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from trl_wrapper.trainer_wrapper import TrainerWrapper, WrapperConfig, SMOL_LM_135M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-28 01:31:01.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mLoading model HuggingFaceTB/SmolLM2-135M-Instruct\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cfg = WrapperConfig(\n",
    "    model_id_or_path=SMOL_LM_135M,\n",
    "    max_samples=1000,\n",
    "    n_epochs=10,\n",
    "    data_module_choice=\"evol_codealpaca_dpo\",\n",
    "    notebook_mode=True,\n",
    "    tuning_mode=\"sft_lora\",\n",
    "    eval_data_mode=\"fixed\"\n",
    ")\n",
    "\n",
    "wrapper = TrainerWrapper(cfg)\n",
    "wrapper.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "! echo $CUDA_VISIBLE_DEVICES\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-28 01:44:00.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mCache dir: ../dataset_caches/evol_code_alpaca_data_module\u001b[0m\n",
      "\u001b[32m2024-12-28 01:44:00.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoading dataset for stage fit\u001b[0m\n",
      "\u001b[32m2024-12-28 01:44:00.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing dataset for stage fit, workers: 1, cache dir ../dataset_caches/evol_code_alpaca_data_module\u001b[0m\n",
      "\u001b[32m2024-12-28 01:44:00.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset.code\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mLoading dataset\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evol_codealpaca_dpo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  13%|█▎        | 5000/39483 [00:00<00:00, 41795.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  43%|████▎     | 17000/39483 [00:00<00:00, 47282.66 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  71%|███████   | 28000/39483 [00:00<00:00, 42883.70 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 39483/39483 [00:00<00:00, 45460.97 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n",
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 39483/39483 [00:00<00:00, 44176.26 examples/s]\n",
      "Map: 100%|██████████| 399/399 [00:00<00:00, 20906.55 examples/s]\n",
      "\u001b[32m2024-12-28 01:44:02.789\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m165\u001b[0m - \u001b[33m\u001b[1mColumns in dataset: ['system', 'question', 'chosen', 'rejected', 'conversations'] do not match torch columns, not setting format\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['system', 'question', 'chosen', 'rejected'])\n",
      "dict_keys(['conversations'])\n"
     ]
    }
   ],
   "source": [
    "wrapper.init_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are experienced in guiding both individual developers and teams, offering mentorship in personal growth, coding skills improvement, and collaborative development.',\n",
       "  'role': 'system'},\n",
       " {'content': \"The similarity measure between two attributes always falls in the range of $[0,1]$, why is this not true in the case of dissimilarity where the value falls in the range of $[0, \\\\infty)$?\\nCan't the dissimilarity between them be thought of as the opposite of similarity in the sense that if they have a similarity of 0, then the dissimilarity will be 1 and vice versa?\",\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.data_module.train_dataset['conversations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-28 01:38:08.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mLoRA already loaded, ignoring\u001b[0m\n",
      "\u001b[32m2024-12-28 01:38:08.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m214\u001b[0m - \u001b[1mInitializing trainer, run: run-12-28-1-38-96911, project: codecontests-llama-3b\u001b[0m\n",
      "\u001b[32m2024-12-28 01:38:08.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m217\u001b[0m - \u001b[1mlogprobs cache location: dataset_caches/evol_code_alpaca_data_module/b68b303b/ref_logprobs_cache peft config: True\u001b[0m\n",
      "\u001b[32m2024-12-28 01:38:08.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mWrapperConfig(model_id_or_path='HuggingFaceTB/SmolLM2-135M-Instruct', notebook_mode=True, max_sequence_length=1512, max_prompt_length=1024, max_eval_sample_length=1024, max_samples=1000, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, gradient_checkpointing=True, root_dir=None, data_module_choice='evol_codealpaca_dpo', wandb_project_name='codecontests-llama-3b', n_epochs=10, max_eval_dataset_size=None, adapter_path=None, dpo_beta=0.1, learning_rate=5e-05, max_grad_norm=0.3, lora_rank=256, lora_dropout=0.05, lora_alpha=128, logprob_precompute_batch_size=16, tuning_mode='sft_lora', eval_data_mode='fixed', eval_steps=500, save_steps=500, using_mistral=False, run_suffix=None)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-brianf/smolmodels/notebooks/../trl_wrapper/trainer_wrapper.py:254: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  self.trainer = SFTTrainer(\n",
      "Map:   0%|          | 0/39483 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/notebooks/../trl_wrapper/trainer_wrapper.py:254\u001b[0m, in \u001b[0;36mTrainerWrapper.init_trainer\u001b[0;34m(self, comment)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtuning_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msft_lora\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    223\u001b[0m     args \u001b[38;5;241m=\u001b[39m SFTConfig(\n\u001b[1;32m    224\u001b[0m         num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_epochs,\n\u001b[1;32m    225\u001b[0m         per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m         disable_tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnotebook_mode,\n\u001b[1;32m    252\u001b[0m     )\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     args \u001b[38;5;241m=\u001b[39m DPOConfig(\n\u001b[1;32m    264\u001b[0m         num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_epochs,\n\u001b[1;32m    265\u001b[0m         per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         disable_tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnotebook_mode,\n\u001b[1;32m    297\u001b[0m     )\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:265\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PartialState()\u001b[38;5;241m.\u001b[39mlocal_main_process_first():\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_of_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars_per_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m         _multiple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:380\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[0;34m(self, dataset, processing_class, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens, skip_prepare_dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m packing:\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_non_packed_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_packed_dataloader(\n\u001b[1;32m    392\u001b[0m         processing_class,\n\u001b[1;32m    393\u001b[0m         dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m         add_special_tokens,\n\u001b[1;32m    401\u001b[0m     )\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:455\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_non_packed_dataloader\u001b[0;34m(self, processing_class, dataset, dataset_text_field, max_seq_length, formatting_func, add_special_tokens, remove_unused_columns)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m    454\u001b[0m     map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_num_proc  \u001b[38;5;66;03m# this arg is not available for IterableDataset\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmap_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_dataset\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3068\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3069\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3070\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3071\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3072\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3073\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3476\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3472\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3473\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3474\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3475\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3476\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3485\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3338\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3337\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3338\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3340\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3341\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3342\u001b[0m     }\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:415\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_non_packed_dataloader.<locals>.tokenize\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(element):\n\u001b[0;32m--> 415\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessing_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m formatting_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatting_func(element), \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `formatting_func` should return a list of processed strings since it can lead to silent bugs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         )\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2860\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2859\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2860\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2862\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/smolmodels/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2920\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2917\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2920\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2921\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2922\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2923\u001b[0m     )\n\u001b[1;32m   2925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2926\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2927\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2928\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2929\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "wrapper.init_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "for prompt, chosen, rejected, logprob_diff in []:\n",
    "    display(Markdown(f\"\\n### Prompt: {prompt}\\n\\n### Chosen:\\n {chosen}\\n\\n### Rejected:\\n {rejected}\\n\\nLogprob diff: {logprob_diff}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

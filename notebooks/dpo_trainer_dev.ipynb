{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianfitzgerald/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from trl_wrapper.trainer_wrapper import TrainerWrapper, GUTENBERG_CONFIG, SMOL_LM_135M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = GUTENBERG_CONFIG\n",
    "cfg.notebook_mode = True\n",
    "cfg.model_id_or_path = SMOL_LM_135M\n",
    "cfg.max_samples = 1000\n",
    "wrapper = TrainerWrapper(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-19 21:10:18.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mLoading model HuggingFaceTB/SmolLM2-135M-Instruct with attn_impl: sdpa\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wrapper.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-19 21:10:35.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_data_module\u001b[0m:\u001b[36m298\u001b[0m - \u001b[1mUsing dataset path: dataset_files/gutenberg_conversations.parquet\u001b[0m\n",
      "\u001b[32m2025-02-19 21:10:35.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mCache dir: ../dataset_caches/conversation_data_module\u001b[0m\n",
      "\u001b[32m2025-02-19 21:10:35.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mLoading dataset for stage fit\u001b[0m\n",
      "\u001b[32m2025-02-19 21:10:35.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mProcessing dataset for stage fit, workers: 1, cache dir ../dataset_caches/conversation_data_module, using cache: False\u001b[0m\n",
      "\u001b[32m2025-02-19 21:10:35.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36minit_dataset\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mLoading local dataset from ../dataset_files/gutenberg_conversations.parquet\u001b[0m\n",
      "Generating train split: 11088 examples [00:00, 146907.76 examples/s]\n",
      "\u001b[32m2025-02-19 21:10:35.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36minit_dataset\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mSelecting 1000 samples\u001b[0m\n",
      "\u001b[32m2025-02-19 21:10:35.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36minit_dataset\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mLoading custom chat template: ../chat_templates/llama3.jinja\u001b[0m\n",
      "\u001b[32m2025-02-19 21:10:35.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.utils\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m171\u001b[0m - \u001b[1mTrain dataset samples: 900 Val dataset samples: 100 Train steps per epoch: 112\u001b[0m\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 1446.99 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1300.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "wrapper.init_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-19 21:10:38.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mSaving output to: ./runs/02-19-21-10-750867-smollm2-135m-instruct--gutenberg-conv\u001b[0m\n",
      "\u001b[32m2025-02-19 21:10:38.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_trainer\u001b[0m:\u001b[36m366\u001b[0m - \u001b[1mInitializing trainer, run_name: 02-19-21-10-750867-smollm2-135m-instruct--gutenberg-conv, wandb project: gutenberg\u001b[0m\n",
      "/Users/brianfitzgerald/Documents/GitHub/smolmodels/notebooks/../trl_wrapper/trainer_wrapper.py:430: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  self.trainer = SFTTrainer(\n",
      "Converting train dataset to ChatML (num_proc=4): 100%|██████████| 900/900 [00:00<00:00, 7477.99 examples/s]\n",
      "Applying chat template to train dataset (num_proc=4): 100%|██████████| 900/900 [00:00<00:00, 3738.40 examples/s]\n",
      "Applying chat template to train dataset (num_proc=4): 100%|██████████| 900/900 [00:00<00:00, 4193.91 examples/s]\n",
      "Converting eval dataset to ChatML (num_proc=4): 100%|██████████| 100/100 [00:00<00:00, 1178.47 examples/s]\n",
      "Applying chat template to eval dataset (num_proc=4): 100%|██████████| 100/100 [00:00<00:00, 437.70 examples/s]\n",
      "Applying chat template to eval dataset (num_proc=4): 100%|██████████| 100/100 [00:00<00:00, 963.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "wrapper.init_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='HuggingFaceTB/SmolLM2-135M-Instruct', vocab_size=49152, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|im_end|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<repo_name>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<reponame>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<file_sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<filename>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<gh_stars>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t11: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t12: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t13: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t14: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t15: AddedToken(\"<jupyter_script>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t16: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

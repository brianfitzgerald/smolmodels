{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "\u001b[32m2025-04-10 21:55:41.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m285\u001b[0m - \u001b[1mSetting padding side to: right\u001b[0m\n",
      "\u001b[32m2025-04-10 21:55:41.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_data_module\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mUsing chat template override: ministral\u001b[0m\n",
      "\u001b[32m2025-04-10 21:55:41.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.wrapper_config\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mCache dir: ../dataset_caches/connections_data_module\u001b[0m\n",
      "Map: 100%|██████████| 100000/100000 [00:04<00:00, 21136.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model.reasoning import ConnectionsDataModule\n",
    "from trl_wrapper.trainer_wrapper import TrainerWrapper, GRPO_MATH_CONFIG, CONNECTIONS_CONFIG\n",
    "from trl_wrapper.wrapper_config import DatasetConfig\n",
    "from IPython.display import display\n",
    "\n",
    "wrapper = TrainerWrapper(CONNECTIONS_CONFIG)\n",
    "\n",
    "wrapper.init_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candle, crayon, honeycomb, seal, defense, excuse, out, reason, kettles, mittens, raindrops, whiskers, canine, fang, molar, tusk\n"
     ]
    }
   ],
   "source": [
    "first_sample = wrapper.data_module.train_dataset[1]\n",
    "print(first_sample['prompt'][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dope', 'scoop', 'skinny', 'word'],\n",
       " ['con', 'dupe', 'fool', 'trick'],\n",
       " ['cant', 'lean', 'list', 'slope'],\n",
       " ['boob', 'eggshell', 'giggle', 'hello']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(fake_response['content'])\n",
    "display(first_sample['answer_formatted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-27 21:10:04.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mXML count rewards: [0.25]\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mStrict format rewards: [0.0]\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mconnections_reward_func\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mModel generations: ['<reasoning>\\nFirst four are academic\\n</reasoning>\\n<answer><group>dope, scoop, skinny, word</group>\\n<group>con, dupe, fool, trick</group>\\n<group>cant, lean, list, slope</group>\\n<group>boob, eggshell, giggle, hello</group></answer>\\n']\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mconnections_reward_func\u001b[0m:\u001b[36m177\u001b[0m - \u001b[1mGroups: [[['dope', 'scoop', 'skinny', 'word'], ['con', 'dupe', 'fool', 'trick'], ['cant', 'lean', 'list', 'slope'], ['boob', 'eggshell', 'giggle', 'hello']]]\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mconnections_reward_func\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mConnections scores: [4.0]\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward_func\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mGroup size rewards: [0.5]\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward_func\u001b[0m:\u001b[36m196\u001b[0m - \u001b[1mNumber of groups rewards: [0.5]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for reward_func in wrapper.data_module.reward_functions():\n",
    "    rew = reward_func([conv], [[fake_response]], answer=first_sample['answer_formatted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

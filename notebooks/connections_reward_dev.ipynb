{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianfitzgerald/Documents/GitHub/smolmodels/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-03 20:03:05,950\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "\u001b[32m2025-04-03 20:03:08.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m305\u001b[0m - \u001b[1mSetting padding side to: right\u001b[0m\n",
      "\u001b[32m2025-04-03 20:03:08.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.trainer_wrapper\u001b[0m:\u001b[36minit_data_module\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mUsing chat template override: ministral\u001b[0m\n",
      "\u001b[32m2025-04-03 20:03:08.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrl_wrapper.wrapper_config\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mCache dir: ../dataset_caches/connections_data_module\u001b[0m\n",
      "Map: 100%|██████████| 100000/100000 [00:05<00:00, 18229.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model.reasoning import ConnectionsDataModule\n",
    "from trl_wrapper.trainer_wrapper import TrainerWrapper, GRPO_MATH_CONFIG, GRPO_CONNECTIONS_CONFIG\n",
    "from trl_wrapper.wrapper_config import DatasetConfig\n",
    "from IPython.display import display\n",
    "\n",
    "wrapper = TrainerWrapper(GRPO_CONNECTIONS_CONFIG)\n",
    "\n",
    "wrapper.init_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groups': [{'reason': 'defeat badly',\n",
       "   'words': ['crush', 'rout', 'shellac', 'trash']},\n",
       "  {'reason': 'tops', 'words': ['cami', 'halter', 'tank', 'tee']},\n",
       "  {'reason': 'butt', 'words': ['bottom', 'buns', 'seat', 'tail']},\n",
       "  {'reason': 'kinds of whales', 'words': ['blue', 'fin', 'gray', 'right']}],\n",
       " 'words': ['crush',\n",
       "  'rout',\n",
       "  'shellac',\n",
       "  'trash',\n",
       "  'cami',\n",
       "  'halter',\n",
       "  'tank',\n",
       "  'tee',\n",
       "  'bottom',\n",
       "  'buns',\n",
       "  'seat',\n",
       "  'tail',\n",
       "  'blue',\n",
       "  'fin',\n",
       "  'gray',\n",
       "  'right'],\n",
       " 'prompt': [{'content': '\\nYou are an expert puzzle solving model.\\nFind groups of words that are related to each other. Each group is four words long. There are exactly four groups in total.\\nYou may only use each word in one group.\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n<group>\\n...\\n</group>\\n<group>\\n...\\n</group>\\n</answer>\\n\\n# Example\\n\\nUser: apple, orange, banana, pear, corolla, charger,\\nAssistant: <reasoning>\\nThe first group are all fruits.\\nThe second group are all cars.\\n</reasoning>\\n<answer>\\n<group>apple, orange, banana, pear</group>\\n<group>corolla, charger</group>\\n</answer>\\n\\n# Example\\n\\nUser: dog, cat, red, white,\\nAssistant: <reasoning>\\nThe first group are all animals.\\nThe second group are all colors.\\n</reasoning>\\n<answer>\\n<group>dog, cat</group>\\n<group>red, white</group>\\n</answer>\\n',\n",
       "   'role': 'system'},\n",
       "  {'content': 'crush, rout, shellac, trash, cami, halter, tank, tee, bottom, buns, seat, tail, blue, fin, gray, right',\n",
       "   'role': 'user'}],\n",
       " 'answer': '<answer><group>crush, rout, shellac, trash</group>\\n<group>cami, halter, tank, tee</group>\\n<group>bottom, buns, seat, tail</group>\\n<group>blue, fin, gray, right</group></answer>',\n",
       " 'answer_groups': [['crush', 'rout', 'shellac', 'trash'],\n",
       "  ['cami', 'halter', 'tank', 'tee'],\n",
       "  ['bottom', 'buns', 'seat', 'tail'],\n",
       "  ['blue', 'fin', 'gray', 'right']]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sample = wrapper.data_module.train_dataset[0]\n",
    "first_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dope', 'scoop', 'skinny', 'word'],\n",
       " ['con', 'dupe', 'fool', 'trick'],\n",
       " ['cant', 'lean', 'list', 'slope'],\n",
       " ['boob', 'eggshell', 'giggle', 'hello']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(fake_response['content'])\n",
    "display(first_sample['answer_formatted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-27 21:10:04.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mxmlcount_reward_func\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mXML count rewards: [0.25]\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mstrict_format_reward_func\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mStrict format rewards: [0.0]\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mconnections_reward_func\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mModel generations: ['<reasoning>\\nFirst four are academic\\n</reasoning>\\n<answer><group>dope, scoop, skinny, word</group>\\n<group>con, dupe, fool, trick</group>\\n<group>cant, lean, list, slope</group>\\n<group>boob, eggshell, giggle, hello</group></answer>\\n']\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mconnections_reward_func\u001b[0m:\u001b[36m177\u001b[0m - \u001b[1mGroups: [[['dope', 'scoop', 'skinny', 'word'], ['con', 'dupe', 'fool', 'trick'], ['cant', 'lean', 'list', 'slope'], ['boob', 'eggshell', 'giggle', 'hello']]]\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mconnections_reward_func\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mConnections scores: [4.0]\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mgroup_size_reward_func\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mGroup size rewards: [0.5]\u001b[0m\n",
      "\u001b[32m2025-03-27 21:10:04.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.reasoning\u001b[0m:\u001b[36mn_groups_reward_func\u001b[0m:\u001b[36m196\u001b[0m - \u001b[1mNumber of groups rewards: [0.5]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for reward_func in wrapper.data_module.reward_functions():\n",
    "    rew = reward_func([conv], [[fake_response]], answer=first_sample['answer_formatted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

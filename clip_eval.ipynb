{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-brianf/smolmodels/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/admin/home-brianf/smolmodels/venv/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import clip\n",
    "from transformers.models.clip.modeling_clip import CLIPModel\n",
    "from transformers.models.clip.processing_clip import CLIPProcessor\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "preprocess = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0_1_0_prompt_a_black_colored_car_.png 0.34232956171035767\n",
      "1 2 0_1_0_prompt_a_black_colored_car_.png 0.29553014039993286\n",
      "1 3 0_1_0_prompt_a_black_colored_car_.png 0.402837336063385\n",
      "1 4 0_1_0_prompt_a_black_colored_car_.png 0.29748958349227905\n",
      "2 1 16_4_0_prompt_a_white_car_and_a_re_.png 0.42547935247421265\n",
      "2 2 16_4_0_prompt_a_white_car_and_a_re_.png 0.40552765130996704\n",
      "2 3 16_4_0_prompt_a_white_car_and_a_re_.png 0.4499005079269409\n",
      "2 4 16_4_0_prompt_a_white_car_and_a_re_.png 0.33845990896224976\n",
      "3 1 0_0_1_upsampled_the_sleek_red_body_o_.png 0.1482393741607666\n",
      "3 2 0_0_1_upsampled_the_sleek_red_body_o_.png 0.16503793001174927\n",
      "3 3 0_0_1_upsampled_the_sleek_red_body_o_.png 0.2807549238204956\n",
      "3 4 0_0_1_upsampled_the_sleek_red_body_o_.png 0.1535899043083191\n",
      "4 1 16_1_0_prompt_a_green_apple_and_a__.png 0.3228135108947754\n",
      "4 2 16_1_0_prompt_a_green_apple_and_a__.png 0.23909389972686768\n",
      "4 3 16_1_0_prompt_a_green_apple_and_a__.png 0.4189155697822571\n",
      "4 4 16_1_0_prompt_a_green_apple_and_a__.png 0.3825032114982605\n",
      "5 1 8_5_1_upsampled_in_a_lush_savannah_a_.png 0.271426796913147\n",
      "5 2 8_5_1_upsampled_in_a_lush_savannah_a_.png 0.26039212942123413\n",
      "5 3 8_5_1_upsampled_in_a_lush_savannah_a_.png 0.24005663394927979\n",
      "5 4 8_5_1_upsampled_in_a_lush_savannah_a_.png 0.2349492907524109\n",
      "6 1 16_3_1_upsampled_a_cozy_study_filled__.png 0.15892714262008667\n",
      "6 2 16_3_1_upsampled_a_cozy_study_filled__.png 0.10694712400436401\n",
      "6 3 16_3_1_upsampled_a_cozy_study_filled__.png 0.20116257667541504\n",
      "6 4 16_3_1_upsampled_a_cozy_study_filled__.png 0.24404782056808472\n",
      "7 1 8_7_0_prompt_a_red_car_and_a_whit_.png 0.28987252712249756\n",
      "7 2 8_7_0_prompt_a_red_car_and_a_whit_.png 0.16563206911087036\n",
      "7 3 8_7_0_prompt_a_red_car_and_a_whit_.png 0.3547084927558899\n",
      "7 4 8_7_0_prompt_a_red_car_and_a_whit_.png 0.30531126260757446\n",
      "8 1 8_0_1_upsampled_a_blackcolored_banan_.png 0.16729986667633057\n",
      "8 2 8_0_1_upsampled_a_blackcolored_banan_.png 0.11724317073822021\n",
      "8 3 8_0_1_upsampled_a_blackcolored_banan_.png 0.2660272717475891\n",
      "8 4 8_0_1_upsampled_a_blackcolored_banan_.png 0.18835300207138062\n",
      "9 1 16_3_0_prompt_a_yellow_book_and_a__.png 0.24445891380310059\n",
      "9 2 16_3_0_prompt_a_yellow_book_and_a__.png 0.18815433979034424\n",
      "9 3 16_3_0_prompt_a_yellow_book_and_a__.png 0.25289106369018555\n",
      "9 4 16_3_0_prompt_a_yellow_book_and_a__.png 0.1965874433517456\n",
      "10 1 0_2_0_prompt_a_pink_colored_car_.png 0.42623716592788696\n",
      "10 2 0_2_0_prompt_a_pink_colored_car_.png 0.2855159044265747\n",
      "10 3 0_2_0_prompt_a_pink_colored_car_.png 0.42343080043792725\n",
      "10 4 0_2_0_prompt_a_pink_colored_car_.png 0.37284964323043823\n",
      "11 1 8_2_1_upsampled_a_sleek_and_modern_s_.png 0.17220300436019897\n",
      "11 2 8_2_1_upsampled_a_sleek_and_modern_s_.png 0.10923647880554199\n",
      "11 3 8_2_1_upsampled_a_sleek_and_modern_s_.png 0.31278347969055176\n",
      "11 4 8_2_1_upsampled_a_sleek_and_modern_s_.png 0.22502577304840088\n",
      "12 1 8_6_0_prompt_a_brown_colored_gira_.png 0.10258370637893677\n",
      "12 2 8_6_0_prompt_a_brown_colored_gira_.png 0.14446532726287842\n",
      "12 3 8_6_0_prompt_a_brown_colored_gira_.png 0.09891527891159058\n",
      "12 4 8_6_0_prompt_a_brown_colored_gira_.png 0.23684585094451904\n",
      "13 1 0_5_1_upsampled_a_bluecolored_dog_wi_.png 0.3867664933204651\n",
      "13 2 0_5_1_upsampled_a_bluecolored_dog_wi_.png 0.19374686479568481\n",
      "13 3 0_5_1_upsampled_a_bluecolored_dog_wi_.png 0.40364933013916016\n",
      "13 4 0_5_1_upsampled_a_bluecolored_dog_wi_.png 0.3791736364364624\n",
      "14 1 8_7_1_upsampled_in_a_surreal_landsca_.png 0.363638699054718\n",
      "14 2 8_7_1_upsampled_in_a_surreal_landsca_.png 0.30473166704177856\n",
      "14 3 8_7_1_upsampled_in_a_surreal_landsca_.png 0.33819371461868286\n",
      "14 4 8_7_1_upsampled_in_a_surreal_landsca_.png 0.2654498219490051\n",
      "15 1 8_6_1_upsampled_in_the_heart_of_a_va_.png 0.12300658226013184\n",
      "15 2 8_6_1_upsampled_in_the_heart_of_a_va_.png 0.1075851321220398\n",
      "15 3 8_6_1_upsampled_in_the_heart_of_a_va_.png 0.16890275478363037\n",
      "15 4 8_6_1_upsampled_in_the_heart_of_a_va_.png 0.22982066869735718\n",
      "16 1 16_1_1_upsampled_a_green_apple_sits_p_.png 0.2268829345703125\n",
      "16 2 16_1_1_upsampled_a_green_apple_sits_p_.png 0.31127607822418213\n",
      "16 3 16_1_1_upsampled_a_green_apple_sits_p_.png 0.3309367895126343\n",
      "16 4 16_1_1_upsampled_a_green_apple_sits_p_.png 0.37419289350509644\n",
      "17 1 0_6_0_prompt_a_green_colored_bana_.png 0.29411745071411133\n",
      "17 2 0_6_0_prompt_a_green_colored_bana_.png 0.061317622661590576\n",
      "17 3 0_6_0_prompt_a_green_colored_bana_.png 0.3591429591178894\n",
      "17 4 0_6_0_prompt_a_green_colored_bana_.png 0.26039373874664307\n",
      "18 1 16_2_0_prompt_a_green_cup_and_a_bl_.png 0.22755151987075806\n",
      "18 2 16_2_0_prompt_a_green_cup_and_a_bl_.png 0.2883710265159607\n",
      "18 3 16_2_0_prompt_a_green_cup_and_a_bl_.png 0.3747662901878357\n",
      "18 4 16_2_0_prompt_a_green_cup_and_a_bl_.png 0.37341612577438354\n",
      "19 1 8_2_0_prompt_a_black_colored_sand_.png 0.31807249784469604\n",
      "19 2 8_2_0_prompt_a_black_colored_sand_.png 0.18501824140548706\n",
      "19 3 8_2_0_prompt_a_black_colored_sand_.png 0.33525949716567993\n",
      "19 4 8_2_0_prompt_a_black_colored_sand_.png 0.25472480058670044\n",
      "20 1 0_3_1_upsampled_a_blackcolored_dog_w_.png 0.2457789182662964\n",
      "20 2 0_3_1_upsampled_a_blackcolored_dog_w_.png 0.21866905689239502\n",
      "20 3 0_3_1_upsampled_a_blackcolored_dog_w_.png 0.3813517689704895\n",
      "20 4 0_3_1_upsampled_a_blackcolored_dog_w_.png 0.345719575881958\n",
      "21 1 16_4_1_upsampled_in_a_surreal_landsca_.png 0.2708042860031128\n",
      "21 2 16_4_1_upsampled_in_a_surreal_landsca_.png 0.3749540448188782\n",
      "21 3 16_4_1_upsampled_in_a_surreal_landsca_.png 0.43220531940460205\n",
      "21 4 16_4_1_upsampled_in_a_surreal_landsca_.png 0.4194318652153015\n",
      "22 1 0_7_1_upsampled_a_juicy_red_banana_w_.png 0.1314203143119812\n",
      "22 2 0_7_1_upsampled_a_juicy_red_banana_w_.png 0.17274534702301025\n",
      "22 3 0_7_1_upsampled_a_juicy_red_banana_w_.png 0.3141431212425232\n",
      "22 4 0_7_1_upsampled_a_juicy_red_banana_w_.png 0.36987966299057007\n",
      "23 1 16_2_1_upsampled_in_a_cozy_living_roo_.png 0.35077112913131714\n",
      "23 2 16_2_1_upsampled_in_a_cozy_living_roo_.png 0.156960129737854\n",
      "23 3 16_2_1_upsampled_in_a_cozy_living_roo_.png 0.2757818102836609\n",
      "23 4 16_2_1_upsampled_in_a_cozy_living_roo_.png 0.19910681247711182\n",
      "24 1 0_3_0_prompt_a_black_colored_dog_.png 0.15944868326187134\n",
      "24 2 0_3_0_prompt_a_black_colored_dog_.png 0.16727936267852783\n",
      "24 3 0_3_0_prompt_a_black_colored_dog_.png 0.44426441192626953\n",
      "24 4 0_3_0_prompt_a_black_colored_dog_.png 0.4913678765296936\n",
      "25 1 16_0_0_prompt_a_blue_bird_and_a_br_.png 0.33572840690612793\n",
      "25 2 16_0_0_prompt_a_blue_bird_and_a_br_.png 0.30204063653945923\n",
      "25 3 16_0_0_prompt_a_blue_bird_and_a_br_.png 0.4030299782752991\n",
      "25 4 16_0_0_prompt_a_blue_bird_and_a_br_.png 0.32879412174224854\n",
      "26 1 8_1_1_upsampled_a_whitecolored_sandw_.png 0.20428836345672607\n",
      "26 2 8_1_1_upsampled_a_whitecolored_sandw_.png 0.17979371547698975\n",
      "26 3 8_1_1_upsampled_a_whitecolored_sandw_.png 0.3647215962409973\n",
      "26 4 8_1_1_upsampled_a_whitecolored_sandw_.png 0.35223859548568726\n",
      "27 1 0_7_0_prompt_a_red_colored_banana_.png 0.17674791812896729\n",
      "27 2 0_7_0_prompt_a_red_colored_banana_.png 0.19847339391708374\n",
      "27 3 0_7_0_prompt_a_red_colored_banana_.png 0.3944903016090393\n",
      "27 4 0_7_0_prompt_a_red_colored_banana_.png 0.22058260440826416\n",
      "28 1 8_4_1_upsampled_in_a_lush_savannah_a_.png 0.1472090482711792\n",
      "28 2 8_4_1_upsampled_in_a_lush_savannah_a_.png 0.17391115427017212\n",
      "28 3 8_4_1_upsampled_in_a_lush_savannah_a_.png 0.1960105299949646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, sample_dir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_dirs[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[1;32m     46\u001b[0m     sample_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 47\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_clip_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistance\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 27\u001b[0m, in \u001b[0;36mcalculate_clip_distance\u001b[0;34m(image_path1, image_path2)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_clip_distance\u001b[39m(image_path1, image_path2):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Load the images\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     image_features1 \u001b[38;5;241m=\u001b[39m \u001b[43mclip_encode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     image_features2 \u001b[38;5;241m=\u001b[39m clip_encode_image(image_path2)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Normalize the features\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 21\u001b[0m, in \u001b[0;36mclip_encode_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m image \u001b[38;5;241m=\u001b[39m preprocess(images\u001b[38;5;241m=\u001b[39m[img], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 21\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_features\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:1045\u001b[0m, in \u001b[0;36mCLIPModel.get_image_features\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1040\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1041\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1042\u001b[0m )\n\u001b[1;32m   1043\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1045\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# pooled_output\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_projection(pooled_output)\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:844\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    841\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    842\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m--> 844\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    852\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:630\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    622\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    623\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    624\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m         output_attentions,\n\u001b[1;32m    628\u001b[0m     )\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:381\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    378\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    380\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 381\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    383\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/torch/nn/modules/normalization.py:197\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlayer_norm(\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
      "File \u001b[0;32m~/smolmodels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1682\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1684\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "def get_number_prefix(filename):\n",
    "    # Split the filename by non-digit characters\n",
    "    parts = ''.join(filter(str.isdigit, filename))\n",
    "    # Convert the extracted digits to an integer\n",
    "    return int(parts) if parts else 0\n",
    "\n",
    "def get_sorted_files(directory):\n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    # Sort the files based on the number prefix using the custom key function\n",
    "    sorted_files = sorted(files, key=get_number_prefix)\n",
    "    return sorted_files\n",
    "\n",
    "def clip_encode_image(image_path):\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "    image = preprocess(images=[img], return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**image)\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "def calculate_clip_distance(image_path1, image_path2):\n",
    "    # Load the images\n",
    "    image_features1 = clip_encode_image(image_path1)\n",
    "    image_features2 = clip_encode_image(image_path2)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features1 = image_features1 / image_features1.norm(dim=-1, keepdim=True)\n",
    "    image_features2 = image_features2 / image_features2.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the distance (cosine similarity)\n",
    "    distance = 1 - torch.nn.functional.cosine_similarity(image_features1, image_features2)\n",
    "    \n",
    "    return distance.item()\n",
    "\n",
    "sample_dirs = ['samples/samples_15', 'samples/samples_15_lq', 'samples/samples_mq', 'samples/samples_xl_lq', 'samples/samples_xl_lq_high_cfg']\n",
    "sample_dir_files = [get_sorted_files(d) for d in sample_dirs]\n",
    "\n",
    "common_files = list(set(sample_dir_files[0]).intersection(*sample_dir_files))\n",
    "for i, file in enumerate(common_files):\n",
    "    baseline_file = f'{sample_dirs[0]}/{file}'\n",
    "    for j, sample_dir in enumerate(sample_dirs[1:]):\n",
    "        sample_file = f'{sample_dir}/{file}'\n",
    "        distance = calculate_clip_distance(baseline_file, sample_file)\n",
    "        print(f'{i+1} {j+1} {file} {distance}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
